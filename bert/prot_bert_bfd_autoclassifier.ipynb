{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db8c101",
   "metadata": {},
   "source": [
    "### Do NOT click RUN ALL... or else you will spend 20 minutes redefining the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bdc7ad-1599-4ccd-8abc-5536833571b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from Bio.Seq import Seq\n",
    "np.random.seed(42)\n",
    "from transformers import TFBertModel, BertTokenizer,BertConfig, AutoTokenizer, TFAutoModelForSequenceClassification, AdamWeightDecay\n",
    "import re\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "import pickle\n",
    "import sys\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eaad3",
   "metadata": {},
   "source": [
    "Code for ProtBERT and BFD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad3e1a",
   "metadata": {},
   "source": [
    "Changes from previous version: I am implementing COVID-19 classification with ProtTrans (Rostlab models) starting with the BFD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc4fbf",
   "metadata": {},
   "source": [
    "# Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "550d85f1-6320-432d-8b20-b6531a0fc845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VHorVHH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QITLKESGPTLVKPTQTLTLTCKLSGFSVNTGGVGVGWIRQPPGKA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGDTFNIYAINWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFNSYAITWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>QVQLVESGGGVVQPGRSLRLSCAASGFTFSTHGMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFRRYAISWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11862</th>\n",
       "      <td>EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11863</th>\n",
       "      <td>QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11864</th>\n",
       "      <td>QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11865</th>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11866</th>\n",
       "      <td>QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11415 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 VHorVHH\n",
       "5      QITLKESGPTLVKPTQTLTLTCKLSGFSVNTGGVGVGWIRQPPGKA...\n",
       "32     QVQLVQSGAEVKKPGSSVKVSCKASGDTFNIYAINWVRQAPGQGLE...\n",
       "33     QVQLVQSGAEVKKPGSSVKVSCKASGGTFNSYAITWVRQAPGQGLE...\n",
       "34     QVQLVESGGGVVQPGRSLRLSCAASGFTFSTHGMHWVRQAPGKGLE...\n",
       "35     QVQLVQSGAEVKKPGSSVKVSCKASGGTFRRYAISWVRQAPGQGLE...\n",
       "...                                                  ...\n",
       "11862  EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...\n",
       "11863  QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...\n",
       "11864  QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...\n",
       "11865  EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...\n",
       "11866  QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...\n",
       "\n",
       "[11415 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/CoV-AbDab_031022.csv\")\n",
    "df = df[[\"VHorVHH\"]]\n",
    "df = df[df[\"VHorVHH\"].apply(lambda x: len(x) <= 138)]\n",
    "df = df[(df.VHorVHH != 'ND')]\n",
    "df\n",
    "# df = df[[\"CDRH3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca827dc2-3d2e-4bbd-80ae-08b1ea26e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/Bio/Seq.py:3482: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVQLVQSGPEVKKPGSSVKVSCKASGGTFSNFAFSWVRQAPGQGLEWMGSVILHLGTSTYAQKFQGRVTITADESTSAAFMDLNALTSDDTAVYYCARVVAVPGRVPYWFDPWGQGTLVTVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARVPPTSTVTTLGDDYWGQGTLVTVSS', 'QVQLVQSGPEVKKPGASVRVSCKPSGYPFSNYGISWMRQAPGQGLEWMGWVNIDKGNTKYAQKFQDRVTMTTDTSSSTVYLELRSLRSDDTALYYCARERGGYRYGDYWGQGTLVIVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEIKHSGSTNYIPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCASRAGAAAASWGQGTLVTVSS', 'SETLSLTCAVHGGSFSDYYWTWIRQPPGKGLEWIGEINHRGGTNYNPSLKSRLNILVDTSKSQFSLKLSSVTAADTAVYFCARERFILIRGLTKYYYYMDVWGKGTTVTVS'] 11415\n"
     ]
    }
   ],
   "source": [
    "dummy = []\n",
    "head = []\n",
    "with open(\"../Data/cAb-rep/cAb-Rep_heavy.nt.txt\") as myfile:\n",
    "    # count = 0\n",
    "    for i in myfile:\n",
    "        # if count <= 1:\n",
    "        #     print(i)\n",
    "        #     if i.find(\">\") == -1 & i.find(\"-\") == -1:\n",
    "        #         print(Seq.translate(i.strip()))\n",
    "        #     count+=1\n",
    "        dummy.append(i)\n",
    "    np.random.shuffle(dummy)\n",
    "    \n",
    "    for i in dummy:\n",
    "        if i.find(\">\") == -1 & i.find(\"-\") == -1 & i.find(\"N\") == -1: # These conditions must be met for a valid sequence, the longest was 141. However, there is no 141 sequence for COVID, the greatest is 138, so we go with that\n",
    "            aa_sequence = Seq.translate(i.strip())\n",
    "            if (len(aa_sequence) <= 138) & (len(aa_sequence) >= 100):\n",
    "                head.append(aa_sequence)\n",
    "                if len(head) >= 11415:\n",
    "                    break\n",
    "print(head[:5], len(head))\n",
    "healthy_sequences = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee3c0779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del head\n",
    "del myfile\n",
    "del dummy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67483afe-f027-4dd9-9d03-e62e4145b446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "covid_sequences = df.to_numpy()\n",
    "covid_sequences = np.squeeze(covid_sequences)\n",
    "np.random.shuffle(covid_sequences)\n",
    "print(len(max(healthy_sequences, key=len)))\n",
    "print(len(max(covid_sequences, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "842f6433-f573-4d33-9280-86f26c1ac116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order from https://www.ncbi.nlm.nih.gov/Class/MLACourse/Modules/MolBioReview/iupac_aa_abbreviations.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "572bbba7",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b745d707-e2ff-4562-9e2d-a602b198b1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "healthy_lables = [0] * 11415\n",
    "covid_lables = [1] * 11415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e4d82be-b95e-4fa8-b69b-8595404f4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((healthy_sequences, covid_sequences))\n",
    "y = np.concatenate((healthy_lables, covid_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1335e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.tolist()\n",
    "y = y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568d6d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del healthy_sequences\n",
    "del covid_sequences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aac39c",
   "metadata": {
    "id": "e0aac39c"
   },
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9099e7c",
   "metadata": {
    "id": "a9099e7c"
   },
   "source": [
    "Since the data we're loading isn't prepared for us as a machine learning dataset, we'll have to split the data into train and test sets ourselves! We can use sklearn's function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7452eec8-9c73-452a-b112-92a653d1fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22372ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "958e8829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVQLVESGGGLIQPGGSLRLSCAASGFTVSSNYMSWVRQAPGKGLEWVSVIYSGGSTYYADSVKGRFTVSRDNSKNTLYLQMNSLRAEDTAVYYCARGGRYDYDVFDIWGQGTMVTVSS\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c91accfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 16:02:57.630387: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-02 16:02:57.630520: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n",
      "maxCacheSize: 24.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd\", do_lower_case=False)\n",
    "# embedding_model = TFBertModel.from_pretrained(\"Rostlab/prot_bert_bfd\", from_pt=True)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"Rostlab/prot_bert_bfd\", num_labels=2, from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "687386af",
   "metadata": {
    "id": "687386af",
    "outputId": "aa4a3278-b638-4b87-c55c-0beaf8fa184b"
   },
   "outputs": [],
   "source": [
    "def tokenize(sequence_list):\n",
    "    sequences = [re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in sequence_list]\n",
    "    sequences = [(\" \".join(s)) for s in sequences]\n",
    "    ids = tokenizer.batch_encode_plus(sequences, padding=True, return_tensors=\"tf\")\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a719808",
   "metadata": {
    "id": "9a719808"
   },
   "source": [
    "This looks good! We can see that our sequence has been converted into `input_ids`, which is the tokenized sequence, and an `attention_mask`. The attention mask handles the case when we have sequences of variable length - in those cases, the shorter sequences are padded with blank \"padding\" tokens, and the attention mask is padded with 0s to indicate that those tokens should be ignored by the model.\n",
    "\n",
    "So now, let's tokenize our whole dataset. Note that we don't need to do anything with the labels, as they're already in the format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56e26ddf",
   "metadata": {
    "id": "56e26ddf"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tokenize(X_train)\n",
    "test_tokenized = tokenize(X_test)\n",
    "val_tokenized = tokenize(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b3a6543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(18492, 140), dtype=int32, numpy=\n",
       "array([[ 2,  9,  8, ...,  0,  0,  0],\n",
       "       [ 2, 18, 10, ...,  0,  0,  0],\n",
       "       [ 2,  7, 10, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 2, 18,  8, ...,  0,  0,  0],\n",
       "       [ 2, 15,  5, ...,  0,  0,  0],\n",
       "       [ 2, 18,  8, ...,  0,  0,  0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(18492, 140), dtype=int32, numpy=\n",
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(18492, 140), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb79ba6c",
   "metadata": {
    "id": "fb79ba6c",
    "outputId": "83cea580-6d6b-4efe-d819-6c44da023763"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 18492\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "val_dataset = Dataset.from_dict(val_tokenized)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e809e47",
   "metadata": {
    "id": "9e809e47"
   },
   "source": [
    "This looks good, but we're missing our labels! Let's add those on as an extra column to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090acc0d",
   "metadata": {
    "id": "090acc0d",
    "outputId": "bf732b0d-e9c1-44b6-9e14-37f099c29ea9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 18492\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", y_train) # train_labels = y_train\n",
    "test_dataset = test_dataset.add_column(\"labels\", y_test)\n",
    "val_dataset = val_dataset.add_column(\"labels\", y_val)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28a791e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  419931136 \n",
      "                                                                 \n",
      " dropout_183 (Dropout)       multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 419,933,186\n",
      "Trainable params: 419,933,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamWeightDecay, TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"Rostlab/prot_bert_bfd\", num_labels=2, from_pt=True)\n",
    "\n",
    "model.compile(optimizer=AdamWeightDecay(2e-5), metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14fcf32d",
   "metadata": {
    "id": "14fcf32d"
   },
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_val_set = model.prepare_tf_dataset(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52c7201b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 16:03:17.076655: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-02 16:03:29.290129: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2311/2311 [==============================] - ETA: 0s - loss: 0.5810 - accuracy: 0.6551"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 16:34:03.114877: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2311/2311 [==============================] - 1979s 840ms/step - loss: 0.5810 - accuracy: 0.6551 - val_loss: 0.4931 - val_accuracy: 0.7392\n",
      "Epoch 2/3\n",
      "2311/2311 [==============================] - 2101s 907ms/step - loss: 0.6233 - accuracy: 0.6060 - val_loss: 0.6940 - val_accuracy: 0.4968\n",
      "Epoch 3/3\n",
      "2311/2311 [==============================] - 2272s 964ms/step - loss: 0.6945 - accuracy: 0.4979 - val_loss: 0.6931 - val_accuracy: 0.5032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x3ce737010>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_set, validation_data=tf_val_set, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca836e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.label2id = {\"healthy\": 0, \"covid\": 1}\n",
    "model.id2label = {val: key for key, val in model.label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868bf45",
   "metadata": {
    "id": "8868bf45"
   },
   "source": [
    "Now we can push it to the hub as simply as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c5bb1",
   "metadata": {
    "id": "770c5bb1",
    "outputId": "5eebae66-43c8-4d6e-c51f-0fd64ab8fe5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/josephyu12/antiberty-finetuned-healthy-covid-classification/commit/d8ece0542e3e08a8ecfce354d138be7c590ccdd9', commit_message='Upload tokenizer', commit_description='', oid='d8ece0542e3e08a8ecfce354d138be7c590ccdd9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"prot_berd_bfd\"\n",
    "finetuned_model_name = f\"{model_name}-finetuned-healthy-covid-classification\"\n",
    "\n",
    "model.push_to_hub(finetuned_model_name)\n",
    "tokenizer.push_to_hub(finetuned_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c38c8779811d1cfaf4b5a784c97578f212c26cffc36ab1ef679f872ba1fdba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
