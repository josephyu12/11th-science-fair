{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method gives me embeddings, I'm not quite sure how since it hasn't been trained. But maybe its the \"Positional Fixed Weights\" part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import convert_to_tensor, string\n",
    "from tensorflow.keras.layers import TextVectorization, Embedding, Layer\n",
    "from tensorflow.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32\n",
    "from tensorflow.keras.layers import Dense, Layer\n",
    "from keras.backend import softmax\n",
    "\n",
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, queries, keys, values, d_k, mask=None):\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling\n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))\n",
    "\n",
    "        # Apply mask to the attention scores\n",
    "        if mask is not None:\n",
    "            scores += -1e9 * mask\n",
    "\n",
    "        # Computing the weights by a softmax operation\n",
    "        weights = softmax(scores)\n",
    "\n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)\n",
    "\n",
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, **kwargs):\n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "        self.attention = DotProductAttention()  # Scaled dot product attention\n",
    "        self.heads = h  # Number of attention heads to use\n",
    "        self.d_k = d_k  # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v  # Dimensionality of the linearly projected values\n",
    "        self.d_model = d_model  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)  # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)  # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)  # Learned projection matrix for the values\n",
    "        self.W_o = Dense(d_model)  # Learned projection matrix for the multi-head output\n",
    "\n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    "\n",
    "    def call(self, queries, keys, values, mask=None):\n",
    "        # Rearrange the queries to be able to compute all heads in parallel\n",
    "        q_reshaped = self.reshape_tensor(self.W_q(queries), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        k_reshaped = self.reshape_tensor(self.W_k(keys), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        v_reshaped = self.reshape_tensor(self.W_v(values), self.heads, True)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask)\n",
    "        # Resulting tensor shape: (batch_size, heads, input_seq_length, -1)\n",
    "\n",
    "        # Rearrange back the output into concatenated form\n",
    "        output = self.reshape_tensor(o_reshaped, self.heads, False)\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_v)\n",
    "\n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        # Resulting tensor shape: (batch_size, input_seq_length, d_model)\n",
    "        return self.W_o(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)                                          \n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    "\n",
    "\n",
    "    def call(self, inputs):        \n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])\n",
    "        embedded_words = self.word_embedding_layer(inputs)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)\n",
    "        return embedded_words + embedded_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LayerNormalization, Layer, Dense, ReLU, Dropout\n",
    "\n",
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    "\n",
    "    def call(self, x, sublayer_x):\n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    "\n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    "\n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    def __init__(self, d_ff, d_model, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(d_ff)  # First fully connected layer\n",
    "        self.fully_connected2 = Dense(d_model)  # Second fully connected layer\n",
    "        self.activation = ReLU()  # ReLU activation layer\n",
    "\n",
    "    def call(self, x):\n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    "\n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    "\n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, h, d_k, d_v, d_model, d_ff, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.multihead_attention = MultiHeadAttention(h, d_k, d_v, d_model)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(d_ff, d_model)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    "\n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    "\n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    "\n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "    def __init__(self, vocab_size, sequence_length, h, d_k, d_v, d_model, d_ff, n, rate, **kwargs):\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, d_model)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.encoder_layer = [EncoderLayer(h, d_k, d_v, d_model, d_ff, rate) for _ in range(n)]\n",
    "\n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, d_model)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    "\n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.71858279e-01 5.05014172e-01 9.11247774e-01 4.46787354e-01\n",
      "  7.67643734e-01]\n",
      " [2.89705721e-02 5.80536926e-01 3.92528032e-01 1.21612501e-01\n",
      "  3.07361170e-01]\n",
      " [7.97548628e-01 7.25644196e-03 9.48710794e-01 2.92264839e-01\n",
      "  7.84302601e-04]\n",
      " [8.30261758e-01 4.45708830e-02 5.24301056e-01 4.53939186e-01\n",
      "  2.14454707e-01]\n",
      " [6.02388910e-01 5.80000084e-01 7.97450372e-01 8.29077755e-01\n",
      "  3.82863051e-01]\n",
      " [4.70218934e-01 6.54556938e-01 1.98918470e-01 5.22202855e-01\n",
      "  7.96010500e-01]\n",
      " [5.67640295e-01 6.51659426e-02 1.40902040e-01 4.47881726e-01\n",
      "  4.46725161e-01]\n",
      " [7.21299670e-01 8.61783045e-01 3.38538774e-01 2.09328571e-01\n",
      "  6.37820082e-02]\n",
      " [9.99232083e-01 8.96083675e-01 9.65027322e-01 2.41734097e-01\n",
      "  6.61406529e-01]\n",
      " [3.18281098e-01 3.89332538e-01 3.49546395e-01 7.99748590e-01\n",
      "  3.81287826e-01]\n",
      " [8.09980526e-01 7.09678378e-01 7.34242152e-01 5.58926495e-01\n",
      "  2.25251634e-01]\n",
      " [2.59889631e-01 8.77667479e-01 3.60072210e-01 2.06520259e-01\n",
      "  3.26376490e-01]\n",
      " [9.45892288e-01 5.18370798e-01 6.33739084e-01 5.11040694e-01\n",
      "  6.66806333e-01]\n",
      " [6.47342524e-01 7.55576071e-01 9.56004293e-01 1.96952847e-01\n",
      "  4.23187720e-01]\n",
      " [7.93983532e-01 1.41849439e-01 2.49921366e-01 5.65237693e-01\n",
      "  2.03507629e-01]\n",
      " [4.83572344e-01 3.48871116e-01 8.12680451e-01 7.55391586e-01\n",
      "  1.85101254e-02]\n",
      " [4.06478771e-02 5.73150341e-01 3.62265428e-01 2.04601815e-01\n",
      "  7.09187733e-01]\n",
      " [8.54748732e-01 4.81613442e-01 9.34425682e-01 6.10897806e-01\n",
      "  9.41430882e-02]\n",
      " [3.28461543e-01 9.67765738e-01 3.51249672e-01 3.59655302e-01\n",
      "  3.36030355e-01]\n",
      " [3.27916261e-01 9.73818581e-01 2.31307725e-01 4.55680317e-01\n",
      "  8.61029746e-01]\n",
      " [1.50020840e-01 5.20109528e-01 7.26914207e-01 5.40933708e-01\n",
      "  4.15283047e-01]\n",
      " [7.47869031e-01 4.38674681e-02 7.52135474e-01 8.41891547e-02\n",
      "  5.38585172e-01]\n",
      " [6.41774095e-01 6.84214950e-02 5.71976431e-01 2.19390913e-01\n",
      "  9.01955535e-01]\n",
      " [8.97369619e-01 3.79622037e-01 6.77528351e-01 3.33183958e-02\n",
      "  6.34277154e-01]\n",
      " [5.86463713e-01 4.74355353e-01 5.69613470e-01 2.25436925e-01\n",
      "  7.23384513e-01]\n",
      " [3.82494147e-01 7.62844274e-02 2.65944317e-01 9.79629154e-01\n",
      "  5.33295428e-01]\n",
      " [2.70297900e-01 8.08809105e-01 2.27641632e-01 1.52854392e-01\n",
      "  3.46577681e-02]\n",
      " [3.53614205e-02 5.61165458e-02 3.37300970e-01 2.19957353e-03\n",
      "  7.72654692e-03]\n",
      " [8.73037328e-01 2.89698875e-01 3.36251606e-01 1.08204188e-01\n",
      "  4.85175123e-01]\n",
      " [5.47826904e-01 2.44028885e-01 8.94214533e-01 2.39582080e-01\n",
      "  1.40065210e-01]\n",
      " [5.21853496e-01 4.69361359e-01 2.44969836e-01 7.10301778e-02\n",
      "  2.42015564e-01]\n",
      " [2.48340064e-01 8.59482144e-01 6.72648981e-01 9.19215564e-01\n",
      "  2.15738587e-01]\n",
      " [7.77217438e-01 3.21386237e-01 3.83450135e-01 4.26626864e-02\n",
      "  4.81467656e-01]\n",
      " [6.49773613e-02 1.10255846e-01 9.52118025e-01 4.15895695e-01\n",
      "  7.14619244e-01]\n",
      " [1.12911674e-01 7.93048047e-01 2.76313073e-01 5.43164871e-01\n",
      "  2.74372043e-01]\n",
      " [2.26608304e-01 1.62037788e-01 3.04036491e-01 1.94159384e-01\n",
      "  9.73992470e-01]\n",
      " [6.25002083e-01 4.78404776e-02 4.12980113e-01 7.00612380e-01\n",
      "  4.72882033e-01]\n",
      " [8.08100143e-01 1.21316015e-02 8.80834440e-01 6.54596238e-01\n",
      "  7.87656683e-01]\n",
      " [7.50557347e-01 9.73914536e-01 4.19790345e-02 5.60873812e-01\n",
      "  6.62382994e-01]\n",
      " [5.88326812e-01 1.76729261e-02 9.44074258e-01 1.67876384e-01\n",
      "  4.58299720e-02]\n",
      " [3.46336995e-02 8.44016494e-01 1.87675297e-02 6.57056945e-01\n",
      "  7.59529839e-02]\n",
      " [6.13753479e-01 4.49619134e-01 5.50088827e-01 8.67816302e-01\n",
      "  1.65526120e-01]\n",
      " [3.11596995e-01 1.08933770e-01 3.46127276e-02 1.85270529e-02\n",
      "  8.67657219e-02]\n",
      " [7.93120535e-03 3.71841680e-01 8.77824076e-01 2.72287261e-01\n",
      "  2.46775369e-02]\n",
      " [3.00990808e-01 2.04055211e-01 4.69044232e-01 2.60489132e-01\n",
      "  2.87535740e-01]\n",
      " [3.23312136e-01 5.48165981e-02 7.46035285e-01 6.04690419e-01\n",
      "  6.24136268e-01]\n",
      " [8.08331845e-01 9.50622883e-01 5.53455816e-01 4.36536531e-01\n",
      "  8.55305852e-01]\n",
      " [5.34015981e-01 8.26463830e-01 8.99713897e-01 3.33421086e-01\n",
      "  7.59780168e-01]\n",
      " [1.42060256e-01 8.48704789e-01 4.78119500e-01 4.43229566e-01\n",
      "  7.20549425e-01]\n",
      " [1.92556019e-01 7.35881235e-01 6.37809439e-01 3.59796017e-01\n",
      "  4.42742963e-01]\n",
      " [7.17655003e-01 9.38733440e-01 1.27395739e-01 8.21877339e-01\n",
      "  9.14585677e-01]\n",
      " [4.24547745e-01 5.85467062e-01 3.80731278e-01 8.97523820e-02\n",
      "  1.68655674e-01]\n",
      " [4.07818262e-01 6.44703901e-01 2.57604792e-01 7.91470056e-01\n",
      "  2.44690572e-01]\n",
      " [4.14280268e-01 7.10904101e-01 2.36660427e-01 3.96083559e-01\n",
      "  3.03506061e-01]\n",
      " [7.06783901e-01 5.55812792e-01 7.09897325e-01 7.15512660e-01\n",
      "  3.78931044e-01]\n",
      " [6.59421235e-01 2.78243805e-01 4.46854807e-01 8.51813890e-01\n",
      "  9.76398550e-01]\n",
      " [8.13546770e-01 7.32432257e-01 3.22229455e-01 4.39076495e-01\n",
      "  8.05854883e-01]\n",
      " [8.07063886e-01 5.50426628e-01 8.21698850e-01 4.64361191e-01\n",
      "  1.73617141e-01]\n",
      " [4.98498576e-01 4.71045395e-02 8.63964732e-01 3.34408277e-01\n",
      "  1.50591130e-01]\n",
      " [7.34476271e-01 2.76457893e-01 6.63539206e-01 3.82799895e-01\n",
      "  9.11007193e-01]\n",
      " [7.13687297e-01 4.23154629e-01 5.58785268e-01 1.04493156e-01\n",
      "  3.35448822e-01]\n",
      " [8.69591013e-01 1.53035954e-01 1.66813446e-01 2.63976733e-01\n",
      "  8.63864682e-01]\n",
      " [5.42549518e-01 6.07342620e-01 1.31831142e-02 7.17438504e-01\n",
      "  4.16928465e-01]\n",
      " [8.10671077e-01 9.16184975e-01 5.81831423e-01 6.17306440e-01\n",
      "  9.74202171e-01]]\n",
      "(64, 5)\n",
      "tf.Tensor(\n",
      "[[[ 9.41725150e-02 -1.29116881e+00  9.21752036e-01 ... -8.24519712e-03\n",
      "    8.01932454e-01 -5.02161741e-01]\n",
      "  [-2.89820641e-01 -1.02515090e+00  1.09101498e+00 ... -5.51454648e-02\n",
      "    1.15356088e+00 -2.34647050e-01]\n",
      "  [-3.93740535e-01 -7.25722432e-01  1.37394094e+00 ... -8.77845362e-02\n",
      "    1.09084964e+00  3.00791431e-02]\n",
      "  [ 3.60244274e-01 -1.39812410e+00  1.69734371e+00 ...  8.95727873e-02\n",
      "    8.59972775e-01 -1.54989421e-01]\n",
      "  [-1.16668031e-01 -1.58356345e+00  1.16332603e+00 ... -1.16668031e-01\n",
      "    1.41469693e+00 -1.05248112e-02]]\n",
      "\n",
      " [[-6.02726817e-01 -4.81149703e-01  1.08040380e+00 ...  3.42057273e-02\n",
      "    1.00375402e+00 -9.46208000e-01]\n",
      "  [-2.32967570e-01 -6.68435812e-01  1.23053658e+00 ...  2.06785486e-03\n",
      "    1.32827210e+00 -3.11314106e-01]\n",
      "  [-5.75277090e-01 -4.58032608e-01  1.02981341e+00 ...  3.85854959e-01\n",
      "    1.37187624e+00 -7.30620086e-01]\n",
      "  [-2.42028087e-01 -7.62051523e-01  1.64959514e+00 ...  5.35763949e-02\n",
      "    1.86428070e+00 -4.60406184e-01]\n",
      "  [-9.35121536e-01 -6.96952462e-01  1.29744315e+00 ... -3.16747695e-01\n",
      "    1.98793042e+00 -6.31772280e-01]]\n",
      "\n",
      " [[ 2.33151317e-01 -9.42881167e-01 -8.33183974e-02 ... -8.33183974e-02\n",
      "    1.02891314e+00 -8.33183974e-02]\n",
      "  [ 1.28267199e-01 -1.07851112e+00 -1.07648902e-01 ... -3.69462222e-02\n",
      "    1.11525619e+00 -2.45791837e-03]\n",
      "  [-3.83771658e-01 -1.33391976e+00  1.64503419e+00 ... -1.73157498e-01\n",
      "    1.15112090e+00 -1.11050689e+00]\n",
      "  [ 1.42998770e-01 -1.15496862e+00  1.26246655e+00 ... -1.62120655e-01\n",
      "    1.59628892e+00 -1.02331370e-01]\n",
      "  [-1.69287980e-01 -1.21533625e-01  1.01102030e+00 ... -1.21533625e-01\n",
      "    1.85675406e+00 -1.21533625e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.94600099e-01 -8.14038455e-01  1.06565702e+00 ...  5.31922400e-01\n",
      "    8.13303471e-01 -5.84775150e-01]\n",
      "  [-7.79660046e-02 -7.20285892e-01  2.88367867e-01 ... -8.19052458e-02\n",
      "    1.24021339e+00 -2.03831241e-01]\n",
      "  [-9.30069983e-01 -4.94783461e-01  1.27769279e+00 ...  1.26867414e-01\n",
      "   -1.01943724e-01 -8.74686897e-01]\n",
      "  [-3.27784300e-01 -1.41316879e+00  7.78536737e-01 ...  4.98699784e-01\n",
      "    9.61358726e-01 -7.46441245e-01]\n",
      "  [-7.73959160e-01 -8.07543933e-01  3.01927954e-01 ...  3.12449902e-01\n",
      "    6.60045624e-01 -2.65985966e-01]]\n",
      "\n",
      " [[-2.47375920e-01 -7.57446945e-01  9.92298722e-01 ... -4.14855301e-01\n",
      "    1.68735397e+00 -5.30382395e-01]\n",
      "  [ 1.30365059e-01 -1.25311363e+00  1.14008665e+00 ... -3.83372098e-01\n",
      "    1.55572772e+00 -5.23941994e-01]\n",
      "  [-1.06723046e+00 -1.26741993e+00  6.87424600e-01 ... -6.08164966e-01\n",
      "    9.42435443e-01  2.45934315e-02]\n",
      "  [-2.91581869e-01 -7.17116222e-02  1.03112817e+00 ... -5.65140769e-02\n",
      "    1.41648781e+00 -3.62180531e-01]\n",
      "  [-9.04176533e-01 -7.38597810e-01  1.50323164e+00 ... -2.44795606e-01\n",
      "    1.02342498e+00 -4.80648518e-01]]\n",
      "\n",
      " [[-1.82333291e-01 -5.99971354e-01  1.12798035e+00 ... -3.98303717e-01\n",
      "    1.25193858e+00 -4.67661500e-01]\n",
      "  [-3.13913077e-01  1.47899002e-01  6.64696097e-01 ... -7.05081046e-01\n",
      "    1.32902503e+00 -3.68014008e-01]\n",
      "  [ 4.07132775e-01 -1.13777220e+00  9.39622998e-01 ... -5.98542392e-01\n",
      "    1.47917986e+00 -3.95883024e-01]\n",
      "  [-6.28316998e-01 -1.17262554e+00  1.39022601e+00 ... -1.01474333e+00\n",
      "    2.21375442e+00  4.11397785e-01]\n",
      "  [-1.21666245e-01 -9.34558451e-01  4.51218545e-01 ... -8.43227744e-01\n",
      "    1.38499963e+00 -1.21666245e-01]]], shape=(64, 5, 1024), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from numpy import random\n",
    "\n",
    "enc_vocab_size = 20 # Vocabulary size for the encoder\n",
    "input_seq_length = 5  # Maximum length of the input sequence\n",
    "h = 8  # Number of self-attention heads\n",
    "d_k = 64  # Dimensionality of the linearly projected queries and keys\n",
    "d_v = 64  # Dimensionality of the linearly projected values\n",
    "d_ff = 2048  # Dimensionality of the inner fully connected layer\n",
    "d_model = 1024  # Dimensionality of the model sub-layers' outputs\n",
    "n = 6  # Number of layers in the encoder stack\n",
    "\n",
    "batch_size = 64  # Batch size from the training process\n",
    "dropout_rate = 0.1  # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "input_seq = random.random((batch_size, input_seq_length))\n",
    "\n",
    "print(input_seq)\n",
    "print(input_seq.shape)\n",
    "\n",
    "encoder = Encoder(enc_vocab_size, input_seq_length, h, d_k, d_v, d_model, d_ff, n, dropout_rate)\n",
    "print(encoder(input_seq, None, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c38c8779811d1cfaf4b5a784c97578f212c26cffc36ab1ef679f872ba1fdba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
