{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paratope Prediction using AntiBERTa\n",
    "\n",
    "This notebook describes how one can fine-tune their own AntiBERTa model using the HuggingFace framework. As a demo, we've included the tokenizer we've used, and a minimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of all the things we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Sequence,\n",
    "    ClassLabel\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER_DIR = \"../antiberta/antibody-tokenizer\"\n",
    "\n",
    "# Initialise a tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(TOKENIZER_DIR, max_len=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing for paratope prediction as a token classification task involves a few steps:\n",
    "* Detecting the actual paratopes from PDB structures (this has already been done for convenience)\n",
    "* Splitting non-redundant sequences (this has already been done for convenience)\n",
    "* Loading them into HuggingFace-compatible `dataset` objects: shown below\n",
    "* Tokenizing the sequences: shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling data into the HF framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in parquet files\n",
    "train_df = pd.read_parquet(\n",
    "    '../antiberta/assets/sabdab_train.parquet'\n",
    ")\n",
    "val_df = pd.read_parquet(\n",
    "    '../antiberta/assets/sabdab_val.parquet'\n",
    ")\n",
    "test_df = pd.read_parquet(\n",
    "    '../antiberta/assets/sabdab_test.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>paratope_labels</th>\n",
       "      <th>paratope_sequence</th>\n",
       "      <th>v_gene</th>\n",
       "      <th>j_gene</th>\n",
       "      <th>pdb</th>\n",
       "      <th>antibody_chains</th>\n",
       "      <th>antigen_type_max</th>\n",
       "      <th>compound</th>\n",
       "      <th>paratope_count_bin</th>\n",
       "      <th>v_gene_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>DIVMTQSPDSLAVSLGERATINCKSSQSVLYSSNNKNYLAWYQQKP...</td>\n",
       "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
       "      <td>------------------------------Y------Y--------...</td>\n",
       "      <td>IGKV4-1</td>\n",
       "      <td>IGKJ2</td>\n",
       "      <td>7k9z</td>\n",
       "      <td>BA</td>\n",
       "      <td>protein</td>\n",
       "      <td>Crystal structure of SARS-CoV-2 receptor bindi...</td>\n",
       "      <td>1</td>\n",
       "      <td>VK9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>SALTQPPSVSGAPGQRVTISCTGSSSNIGAGYDVHWYQQLPGTAPK...</td>\n",
       "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
       "      <td>-----------------------------AGYD-------------...</td>\n",
       "      <td>IGLV1-40</td>\n",
       "      <td>IGLJ7</td>\n",
       "      <td>5ush</td>\n",
       "      <td>DE</td>\n",
       "      <td>protein</td>\n",
       "      <td>Structure of vaccinia virus D8 protein bound t...</td>\n",
       "      <td>0</td>\n",
       "      <td>VL8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>VLTQPPSASGTPGQRVTISCSGSNSNIATNYVCWYQQYPGTAPKPL...</td>\n",
       "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
       "      <td>----------------------------TNY---------------...</td>\n",
       "      <td>IGLV1-47</td>\n",
       "      <td>IGLJ3</td>\n",
       "      <td>6xqw</td>\n",
       "      <td>HL</td>\n",
       "      <td>protein</td>\n",
       "      <td>Crystal Structure of MaliM03 Fab in complex wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>VL8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  \\\n",
       "1002  DIVMTQSPDSLAVSLGERATINCKSSQSVLYSSNNKNYLAWYQQKP...   \n",
       "511   SALTQPPSVSGAPGQRVTISCTGSSSNIGAGYDVHWYQQLPGTAPK...   \n",
       "911   VLTQPPSASGTPGQRVTISCSGSNSNIATNYVCWYQQYPGTAPKPL...   \n",
       "\n",
       "                                        paratope_labels  \\\n",
       "1002  [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...   \n",
       "511   [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...   \n",
       "911   [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...   \n",
       "\n",
       "                                      paratope_sequence    v_gene j_gene  \\\n",
       "1002  ------------------------------Y------Y--------...   IGKV4-1  IGKJ2   \n",
       "511   -----------------------------AGYD-------------...  IGLV1-40  IGLJ7   \n",
       "911   ----------------------------TNY---------------...  IGLV1-47  IGLJ3   \n",
       "\n",
       "       pdb antibody_chains antigen_type_max  \\\n",
       "1002  7k9z              BA          protein   \n",
       "511   5ush              DE          protein   \n",
       "911   6xqw              HL          protein   \n",
       "\n",
       "                                               compound  paratope_count_bin  \\\n",
       "1002  Crystal structure of SARS-CoV-2 receptor bindi...                   1   \n",
       "511   Structure of vaccinia virus D8 protein bound t...                   0   \n",
       "911   Crystal Structure of MaliM03 Fab in complex wi...                   1   \n",
       "\n",
       "     v_gene_cluster  \n",
       "1002            VK9  \n",
       "511             VL8  \n",
       "911             VL8  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a preview\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Dataset Dict with the sequence and paratope labels\n",
    "ab_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[['sequence','paratope_labels']]),\n",
    "    \"validation\": Dataset.from_pandas(val_df[['sequence','paratope_labels']]),\n",
    "    \"test\": Dataset.from_pandas(test_df[['sequence','paratope_labels']])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'paratope_labels', '__index_level_0__'],\n",
       "        num_rows: 720\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequence', 'paratope_labels', '__index_level_0__'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'paratope_labels', '__index_level_0__'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what a DatasetDict object looks like with its individual Dataset things\n",
    "ab_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIVMTQSPDSLAVSLGERATINCKSSQSVLYSSNNKNYLAWYQQKPGQPPKLLIYWASTRESGVPDRFSGSGSGTDFTLTISSLQAEDVAVYYCQQYYSTPPTFGQGTKLEIK']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_dataset['train'].select(range(1))['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "print(ab_dataset['train'].select(range(1))['paratope_labels'][0][20:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': Value(dtype='string', id=None),\n",
       " 'paratope_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the Features of each column in the train dataset within the ab_dataset DatasetDict\n",
    "ab_dataset['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the `paratope_labels` column into a set of `ClassLabel`s, which will be predicted via the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ClassLabel feature which will replace paratope_labels later.\n",
    "paratope_class_label = ClassLabel(2, names=['N','P'])\n",
    "new_feature = Sequence(\n",
    "    paratope_class_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41889b146dda458da6870e0d37a3d935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e1ace5b766a41cf8fa416fd6f3baf2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8665e59ee974fcab9b5e41ed24f7af5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We iterate through the sequence and labels columns\n",
    "# Keeping the sequence column as-is, but applying a str2int function, allowing us to cast later\n",
    "ab_dataset_featurised = ab_dataset.map(\n",
    "    lambda seq, labels: {\n",
    "        \"sequence\": seq,\n",
    "        \"paratope_labels\": [paratope_class_label.str2int(sample) for sample in labels]\n",
    "    }, \n",
    "    input_columns=[\"sequence\", \"paratope_labels\"], batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': Value(dtype='string', id=None),\n",
       " 'paratope_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the old Features instance from the previous ab_dataset\n",
    "# Notice how labels is a Sequence of Value\n",
    "feature_set_copy = ab_dataset['train'].features.copy()\n",
    "feature_set_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to the `new_feature` that we made earlier\n",
    "feature_set_copy['paratope_labels'] = new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20c82d918614346a7099cb95d4bc82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd4942dda50410d9877295e56cac913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5e5c0216e1d436a8c4c7b2c0c437ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ab_dataset_featurised = ab_dataset_featurised.cast(feature_set_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': Value(dtype='string', id=None),\n",
       " 'paratope_labels': Sequence(feature=ClassLabel(names=['N', 'P'], id=None), length=-1, id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_dataset_featurised['train'].features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# now the labels are actually a series of integers, but is recognised by huggingface as a series of Classlabels\n",
    "print(ab_dataset_featurised['train'].select(range(1))['paratope_labels'][0][20:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to redefine this to e.g. put -100 labels for the start/end tokens\n",
    "\n",
    "def preprocess(batch):\n",
    "    # :facepalm: The preprocess function takes tokenizer and needs a LIST not a PT tensor :eyeroll:\n",
    "    # https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=vc0BSBLIIrJQ\n",
    "    \n",
    "    t_inputs = tokenizer(batch['sequence'], \n",
    "        padding=\"max_length\")\n",
    "    batch['input_ids'] = t_inputs.input_ids\n",
    "    batch['attention_mask'] = t_inputs.attention_mask\n",
    "    \n",
    "    # enumerate \n",
    "    labels_container = []\n",
    "    for index, labels in enumerate(batch['paratope_labels']):\n",
    "        \n",
    "        # This is typically length of the sequence + SOS + EOS + PAD (to longest example in batch)\n",
    "        tokenized_input_length = len(batch['input_ids'][index])\n",
    "        paratope_label_length  = len(batch['paratope_labels'][index])\n",
    "        \n",
    "        # we subtract 1 because we start with SOS\n",
    "        # we should in theory have at least 1 \"pad_with_eos\" because an EOS wouldn't have been accounted for in the\n",
    "        # paratope_labels column even for the longest possible sequence\n",
    "        n_pads_with_eos = max(1, tokenized_input_length - paratope_label_length - 1)\n",
    "        \n",
    "        # We have a starting -100 for the SOS\n",
    "        # and fill the rest of seq length with -100 to account for any extra pads and the final EOS token\n",
    "        # The -100s are ignored in the CE loss function\n",
    "        labels_padded = [-100] + labels + [-100] * n_pads_with_eos\n",
    "        \n",
    "        assert len(labels_padded) == len(batch['input_ids'][index]), \\\n",
    "        f\"Lengths don't align, {len(labels_padded)}, {len(batch['input_ids'][index])}, {len(labels)}\"\n",
    "        \n",
    "        labels_container.append(labels_padded)\n",
    "    \n",
    "    # We create a new column called `labels`, which is recognised by the HF trainer object\n",
    "    batch['labels'] = labels_container\n",
    "    \n",
    "    for i,v in enumerate(batch['labels']):\n",
    "        assert len(batch['input_ids'][i]) == len(v) == len(batch['attention_mask'][i])\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58b8ae9a774e4ed9928d6e31701121fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662b4c7f4af24942aa7137d7773533d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba696291eba24ed7b25d3eed49cd66d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply that function above on the dataset - labels now aligned!\n",
    "ab_dataset_tokenized = ab_dataset_featurised.map(\n",
    "    preprocess, \n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=['sequence', 'paratope_labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model set-up and training\n",
    "\n",
    "Here we define:\n",
    "* The callback function to compute some metrics during training (and can be used for evaluation!)\n",
    "* The training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has the actual names that maps 0->N and 1->P\n",
    "label_list = paratope_class_label.names\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    A callback added to the trainer so that we calculate various metrics via sklearn\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    \n",
    "    # The predictions are logits, so we apply softmax to get the probabilities. We only need\n",
    "    # the probabilities of the paratope label, which is index 1 (according to the ClassLabel we made earlier),\n",
    "    # or the last column from the output tensor\n",
    "    prediction_pr = torch.softmax(torch.from_numpy(predictions), dim=2).detach().numpy()[:,:,-1]\n",
    "    \n",
    "    # We run an argmax to get the label\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Only compute on positions that are not labelled -100\n",
    "    preds = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    labs = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    probs = [ \n",
    "        [prediction_pr[i][pos] for (pr, (pos, l)) in zip(prediction, enumerate(label)) if l!=-100]\n",
    "         for i, (prediction, label) in enumerate(zip(predictions, labels)) \n",
    "    ] \n",
    "            \n",
    "    # flatten\n",
    "    preds = sum(preds, [])\n",
    "    labs = sum(labs, [])\n",
    "    probs = sum(probs,[])\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(labs, preds, pos_label=\"P\"),\n",
    "        \"recall\": recall_score(labs, preds, pos_label=\"P\"),\n",
    "        \"f1\": f1_score(labs, preds, pos_label=\"P\"),\n",
    "        \"auc\": roc_auc_score(labs, probs),\n",
    "        \"aupr\": average_precision_score(labs, probs, pos_label=\"P\"),\n",
    "        \"mcc\": matthews_corrcoef(labs, preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size, metric you want etc. \n",
    "batch_size = 32\n",
    "RUN_ID = \"paratope-prediction-task\"\n",
    "SEED = 0\n",
    "LR = 1e-6\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{RUN_ID}_{SEED}\", # this is the name of the checkpoint folder\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=LR, # 1e-6, 5e-6, 1e-5. .... 1e-3\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0, # 0, 0.05, 0.1 .... \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type='linear',\n",
    "    metric_for_best_model='aupr', # name of the metric here should correspond to metrics defined in compute_metrics\n",
    "    logging_strategy='epoch',\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set all seeds to make results reproducible (deterministic mode).\n",
    "    When seed is None, disables deterministic mode.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../antiberta/saved_model were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at ../antiberta/saved_model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# Name of the pre-trained model after you train your MLM\n",
    "MODEL_DIR = \"../antiberta/saved_model\"\n",
    "\n",
    "# We initialise a model using the weights from the pre-trained model\n",
    "model = RobertaForTokenClassification.from_pretrained(MODEL_DIR, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ab_dataset_tokenized['train'],\n",
    "    eval_dataset=ab_dataset_tokenized['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 720\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 230\n",
      "  Number of trainable parameters = 85194242\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83bd14eb8e96484792f275e024642c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/230 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4354, 'learning_rate': 9e-07, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c216a60b252446689c3f847f417b0f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-23\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-23/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.30869412422180176, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_auc': 0.6220205051672912, 'eval_aupr': 0.17721246188876694, 'eval_mcc': 0.0, 'eval_runtime': 3.2295, 'eval_samples_per_second': 27.868, 'eval_steps_per_second': 0.929, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-23/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-23/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-23/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3053, 'learning_rate': 8e-07, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5616adfafeb490b8a7c2a12194b89cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-46\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-46/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2835576832294464, 'eval_precision': 0.0, 'eval_recall': 0.0, 'eval_f1': 0.0, 'eval_auc': 0.7457842502166135, 'eval_aupr': 0.27236866965280115, 'eval_mcc': 0.0, 'eval_runtime': 3.2514, 'eval_samples_per_second': 27.681, 'eval_steps_per_second': 0.923, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-46/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-46/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-46/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2868, 'learning_rate': 7e-07, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b65e0ab5bd96425898456b1d5ab6eb48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-69\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-69/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2671515643596649, 'eval_precision': 0.6129032258064516, 'eval_recall': 0.019487179487179488, 'eval_f1': 0.03777335984095428, 'eval_auc': 0.7881085882812777, 'eval_aupr': 0.3128190229803789, 'eval_mcc': 0.0973336925787922, 'eval_runtime': 3.2396, 'eval_samples_per_second': 27.781, 'eval_steps_per_second': 0.926, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-69/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-69/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-69/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2756, 'learning_rate': 6e-07, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d89e2cef0724787b30d3b6e587b4991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-92\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-92/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2591077983379364, 'eval_precision': 0.6285714285714286, 'eval_recall': 0.022564102564102566, 'eval_f1': 0.04356435643564357, 'eval_auc': 0.8129165607515463, 'eval_aupr': 0.3456545575264204, 'eval_mcc': 0.10656719338387169, 'eval_runtime': 3.2156, 'eval_samples_per_second': 27.988, 'eval_steps_per_second': 0.933, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-92/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-92/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-92/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2673, 'learning_rate': 5e-07, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7100ab2c7494d83a9f4b0debd458a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-115\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-115/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2533701956272125, 'eval_precision': 0.651685393258427, 'eval_recall': 0.059487179487179485, 'eval_f1': 0.10902255639097744, 'eval_auc': 0.8281455970742827, 'eval_aupr': 0.364978905808375, 'eval_mcc': 0.1777510354708629, 'eval_runtime': 3.2693, 'eval_samples_per_second': 27.529, 'eval_steps_per_second': 0.918, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-115/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-115/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-115/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2607, 'learning_rate': 4e-07, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305ebacf11ff45bea5dbca8c74be301a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-138\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-138/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24631254374980927, 'eval_precision': 0.6206896551724138, 'eval_recall': 0.07384615384615385, 'eval_f1': 0.13198900091659027, 'eval_auc': 0.8375128123129403, 'eval_aupr': 0.37705011384115933, 'eval_mcc': 0.1919007414543019, 'eval_runtime': 3.6481, 'eval_samples_per_second': 24.67, 'eval_steps_per_second': 0.822, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-138/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-138/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-138/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2576, 'learning_rate': 3e-07, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e489dfe99e949bc9a86caed78ae1b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-161\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-161/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24151334166526794, 'eval_precision': 0.6394557823129252, 'eval_recall': 0.09641025641025641, 'eval_f1': 0.16755793226381463, 'eval_auc': 0.8431678870821818, 'eval_aupr': 0.38317859963716644, 'eval_mcc': 0.22406481912233273, 'eval_runtime': 3.4895, 'eval_samples_per_second': 25.792, 'eval_steps_per_second': 0.86, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-161/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-161/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-161/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2545, 'learning_rate': 2e-07, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa233d6e32c04964a9a96190263c0721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-184\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-184/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23990122973918915, 'eval_precision': 0.6319444444444444, 'eval_recall': 0.09333333333333334, 'eval_f1': 0.16264521894548703, 'eval_auc': 0.8477366417116977, 'eval_aupr': 0.38823237175583547, 'eval_mcc': 0.2186798046597585, 'eval_runtime': 3.3852, 'eval_samples_per_second': 26.586, 'eval_steps_per_second': 0.886, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-184/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-184/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-184/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2532, 'learning_rate': 1e-07, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7e003f518942b5877b11fe5a42abae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-207\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-207/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23839563131332397, 'eval_precision': 0.6363636363636364, 'eval_recall': 0.10051282051282051, 'eval_f1': 0.1736049601417183, 'eval_auc': 0.8498940848796941, 'eval_aupr': 0.3905218212455077, 'eval_mcc': 0.2281154224859304, 'eval_runtime': 3.2812, 'eval_samples_per_second': 27.429, 'eval_steps_per_second': 0.914, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-207/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-207/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-207/special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.253, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4640f3dc60b643728353258bb4974e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to paratope-prediction-task_0/checkpoint-230\n",
      "Configuration saved in paratope-prediction-task_0/checkpoint-230/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.23847761750221252, 'eval_precision': 0.6423841059602649, 'eval_recall': 0.09948717948717949, 'eval_f1': 0.17229129662522202, 'eval_auc': 0.8506566737458967, 'eval_aupr': 0.3915655146767724, 'eval_mcc': 0.22835709827028988, 'eval_runtime': 3.3002, 'eval_samples_per_second': 27.271, 'eval_steps_per_second': 0.909, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in paratope-prediction-task_0/checkpoint-230/pytorch_model.bin\n",
      "tokenizer config file saved in paratope-prediction-task_0/checkpoint-230/tokenizer_config.json\n",
      "Special tokens file saved in paratope-prediction-task_0/checkpoint-230/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from paratope-prediction-task_0/checkpoint-230 (score: 0.3915655146767724).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1016.3631, 'train_samples_per_second': 7.084, 'train_steps_per_second': 0.226, 'train_loss': 0.2849534034729004, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=230, training_loss=0.2849534034729004, metrics={'train_runtime': 1016.3631, 'train_samples_per_second': 7.084, 'train_steps_per_second': 0.226, 'train_loss': 0.2849534034729004, 'epoch': 10.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# watch stuff fly\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: __index_level_0__. If __index_level_0__ are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 90\n",
      "  Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb1e6e13a35440bb3e8eb69d4ab5e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# run prediction on the test set\n",
    "pred = trainer.predict(\n",
    "    ab_dataset_tokenized['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.2323971837759018,\n",
       " 'test_precision': 0.6686046511627907,\n",
       " 'test_recall': 0.11734693877551021,\n",
       " 'test_f1': 0.1996527777777778,\n",
       " 'test_auc': 0.863730895333623,\n",
       " 'test_aupr': 0.42898343940829897,\n",
       " 'test_mcc': 0.2547548299990841,\n",
       " 'test_runtime': 3.2968,\n",
       " 'test_samples_per_second': 27.3,\n",
       " 'test_steps_per_second': 0.91}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this stores a JSON with metric values\n",
    "pred.metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - how to go from sequence to predicted sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "paratope-prediction-task_0 does not appear to have a file named config.json. Checkout 'https://huggingface.co/paratope-prediction-task_0/None' for available files.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# input sequence of tralokinumab Light chain\u001b[39;00m\n\u001b[1;32m      2\u001b[0m input_seq \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mYVLTQPPSVSVAPGKTARITCGGNIIGSKLVHWYQQKPGQAPVLVIYDDGDRPSGIPERFSGSNSGNTATLTISRVEAGDEADYYCQVWDTGSDPVVFGGGTKLTVL\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m RobertaForTokenClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      4\u001b[0m     \u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mRUN_ID\u001b[39m}\u001b[39;49;00m\u001b[39m_\u001b[39;49m\u001b[39m{\u001b[39;49;00mSEED\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m tokenized_input \u001b[39m=\u001b[39m tokenizer([input_seq], return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m predicted_logits \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtokenized_input)\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/modeling_utils.py:1966\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   1965\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 1966\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   1967\u001b[0m         config_path,\n\u001b[1;32m   1968\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1969\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1970\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1971\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1972\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1973\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1974\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1975\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1976\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1977\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   1978\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   1979\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1980\u001b[0m     )\n\u001b[1;32m   1981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1982\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/configuration_utils.py:532\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    534\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    535\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/configuration_utils.py:559\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    558\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    561\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/configuration_utils.py:614\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    610\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    612\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    615\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    616\u001b[0m         configuration_file,\n\u001b[1;32m    617\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    618\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    619\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    620\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    621\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    622\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    623\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    624\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    625\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    626\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    627\u001b[0m     )\n\u001b[1;32m    628\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    629\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    631\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/utils/hub.py:380\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(resolved_file):\n\u001b[1;32m    379\u001b[0m     \u001b[39mif\u001b[39;00m _raise_exceptions_for_missing_entries:\n\u001b[0;32m--> 380\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    381\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m does not appear to have a file named \u001b[39m\u001b[39m{\u001b[39;00mfull_filename\u001b[39m}\u001b[39;00m\u001b[39m. Checkout \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    382\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available files.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m         )\n\u001b[1;32m    384\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: paratope-prediction-task_0 does not appear to have a file named config.json. Checkout 'https://huggingface.co/paratope-prediction-task_0/None' for available files."
     ]
    }
   ],
   "source": [
    "# input sequence of tralokinumab Light chain\n",
    "input_seq = 'YVLTQPPSVSVAPGKTARITCGGNIIGSKLVHWYQQKPGQAPVLVIYDDGDRPSGIPERFSGSNSGNTATLTISRVEAGDEADYYCQVWDTGSDPVVFGGGTKLTVL'\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    f\"{RUN_ID}_{SEED}\"\n",
    ")\n",
    "\n",
    "tokenized_input = tokenizer([input_seq], return_tensors='pt', padding=True)\n",
    "predicted_logits = model(**tokenized_input)\n",
    "\n",
    "# Simple argmax - no thresholding.\n",
    "argmax = predicted_logits[0].argmax(2)[0][1:-1].cpu().numpy()\n",
    "indices = np.argwhere(argmax).flatten()\n",
    "\n",
    "predicted_sequence = ''\n",
    "\n",
    "for i, s in enumerate(input_seq):\n",
    "    if i in indices:\n",
    "        predicted_sequence += s\n",
    "    else:\n",
    "        predicted_sequence += '-'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c38c8779811d1cfaf4b5a784c97578f212c26cffc36ab1ef679f872ba1fdba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
