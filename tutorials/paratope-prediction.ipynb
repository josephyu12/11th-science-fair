{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paratope Prediction using AntiBERTa\n",
    "\n",
    "This notebook describes how one can fine-tune their own AntiBERTa model using the HuggingFace framework. As a demo, we've included the tokenizer we've used, and a minimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of all the things we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from datasets import (\n",
    "    Dataset,\n",
    "    DatasetDict,\n",
    "    Sequence,\n",
    "    ClassLabel\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    matthews_corrcoef,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "TOKENIZER_DIR = \"../antiberta/antibody-tokenizer\"\n",
    "\n",
    "# Initialise a tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(TOKENIZER_DIR, max_len=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data pre-processing for paratope prediction as a token classification task involves a few steps:\n",
    "* Detecting the actual paratopes from PDB structures (this has already been done for convenience)\n",
    "* Splitting non-redundant sequences (this has already been done for convenience)\n",
    "* Loading them into HuggingFace-compatible `dataset` objects: shown below\n",
    "* Tokenizing the sequences: shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling data into the HF framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in parquet files\n",
    "train_df = pd.read_parquet(\n",
    "    '../antiberta/assets/sabdab_train.parquet'\n",
    ")\n",
    "val_df = pd.read_parquet(\n",
    "    '../antiberta/assets/sabdab_val.parquet'\n",
    ")\n",
    "test_df = pd.read_parquet(\n",
    "    '../antiberta/assets/sabdab_test.parquet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequence</th>\n",
       "      <th>paratope_labels</th>\n",
       "      <th>paratope_sequence</th>\n",
       "      <th>v_gene</th>\n",
       "      <th>j_gene</th>\n",
       "      <th>pdb</th>\n",
       "      <th>antibody_chains</th>\n",
       "      <th>antigen_type_max</th>\n",
       "      <th>compound</th>\n",
       "      <th>paratope_count_bin</th>\n",
       "      <th>v_gene_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>DIVMTQSPDSLAVSLGERATINCKSSQSVLYSSNNKNYLAWYQQKP...</td>\n",
       "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
       "      <td>------------------------------Y------Y--------...</td>\n",
       "      <td>IGKV4-1</td>\n",
       "      <td>IGKJ2</td>\n",
       "      <td>7k9z</td>\n",
       "      <td>BA</td>\n",
       "      <td>protein</td>\n",
       "      <td>Crystal structure of SARS-CoV-2 receptor bindi...</td>\n",
       "      <td>1</td>\n",
       "      <td>VK9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511</th>\n",
       "      <td>SALTQPPSVSGAPGQRVTISCTGSSSNIGAGYDVHWYQQLPGTAPK...</td>\n",
       "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
       "      <td>-----------------------------AGYD-------------...</td>\n",
       "      <td>IGLV1-40</td>\n",
       "      <td>IGLJ7</td>\n",
       "      <td>5ush</td>\n",
       "      <td>DE</td>\n",
       "      <td>protein</td>\n",
       "      <td>Structure of vaccinia virus D8 protein bound t...</td>\n",
       "      <td>0</td>\n",
       "      <td>VL8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>911</th>\n",
       "      <td>VLTQPPSASGTPGQRVTISCSGSNSNIATNYVCWYQQYPGTAPKPL...</td>\n",
       "      <td>[N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...</td>\n",
       "      <td>----------------------------TNY---------------...</td>\n",
       "      <td>IGLV1-47</td>\n",
       "      <td>IGLJ3</td>\n",
       "      <td>6xqw</td>\n",
       "      <td>HL</td>\n",
       "      <td>protein</td>\n",
       "      <td>Crystal Structure of MaliM03 Fab in complex wi...</td>\n",
       "      <td>1</td>\n",
       "      <td>VL8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               sequence  \\\n",
       "1002  DIVMTQSPDSLAVSLGERATINCKSSQSVLYSSNNKNYLAWYQQKP...   \n",
       "511   SALTQPPSVSGAPGQRVTISCTGSSSNIGAGYDVHWYQQLPGTAPK...   \n",
       "911   VLTQPPSASGTPGQRVTISCSGSNSNIATNYVCWYQQYPGTAPKPL...   \n",
       "\n",
       "                                        paratope_labels  \\\n",
       "1002  [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...   \n",
       "511   [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...   \n",
       "911   [N, N, N, N, N, N, N, N, N, N, N, N, N, N, N, ...   \n",
       "\n",
       "                                      paratope_sequence    v_gene j_gene  \\\n",
       "1002  ------------------------------Y------Y--------...   IGKV4-1  IGKJ2   \n",
       "511   -----------------------------AGYD-------------...  IGLV1-40  IGLJ7   \n",
       "911   ----------------------------TNY---------------...  IGLV1-47  IGLJ3   \n",
       "\n",
       "       pdb antibody_chains antigen_type_max  \\\n",
       "1002  7k9z              BA          protein   \n",
       "511   5ush              DE          protein   \n",
       "911   6xqw              HL          protein   \n",
       "\n",
       "                                               compound  paratope_count_bin  \\\n",
       "1002  Crystal structure of SARS-CoV-2 receptor bindi...                   1   \n",
       "511   Structure of vaccinia virus D8 protein bound t...                   0   \n",
       "911   Crystal Structure of MaliM03 Fab in complex wi...                   1   \n",
       "\n",
       "     v_gene_cluster  \n",
       "1002            VK9  \n",
       "511             VL8  \n",
       "911             VL8  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a preview\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Dataset Dict with the sequence and paratope labels\n",
    "ab_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df[['sequence','paratope_labels']]),\n",
    "    \"validation\": Dataset.from_pandas(val_df[['sequence','paratope_labels']]),\n",
    "    \"test\": Dataset.from_pandas(test_df[['sequence','paratope_labels']])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sequence', 'paratope_labels', '__index_level_0__'],\n",
       "        num_rows: 720\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sequence', 'paratope_labels', '__index_level_0__'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sequence', 'paratope_labels', '__index_level_0__'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is what a DatasetDict object looks like with its individual Dataset things\n",
    "ab_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DIVMTQSPDSLAVSLGERATINCKSSQSVLYSSNNKNYLAWYQQKPGQPPKLLIYWASTRESGVPDRFSGSGSGTDFTLTISSLQAEDVAVYYCQQYYSTPPTFGQGTKLEIK']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_dataset['train'].select(range(1))['sequence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'N', 'P', 'N', 'N', 'N', 'N']\n"
     ]
    }
   ],
   "source": [
    "print(ab_dataset['train'].select(range(1))['paratope_labels'][0][20:35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': Value(dtype='string', id=None),\n",
       " 'paratope_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the Features of each column in the train dataset within the ab_dataset DatasetDict\n",
    "ab_dataset['train'].features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the `paratope_labels` column into a set of `ClassLabel`s, which will be predicted via the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ClassLabel feature which will replace paratope_labels later.\n",
    "paratope_class_label = ClassLabel(2, names=['N','P'])\n",
    "new_feature = Sequence(\n",
    "    paratope_class_label\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cf05f3ff13413daf3f97c942fb6386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d48619a676e4cb88ba3605da11298ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bc3061483a4379a0d33ad258a42f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We iterate through the sequence and labels columns\n",
    "# Keeping the sequence column as-is, but applying a str2int function, allowing us to cast later\n",
    "ab_dataset_featurised = ab_dataset.map(\n",
    "    lambda seq, labels: {\n",
    "        \"sequence\": seq,\n",
    "        \"paratope_labels\": [paratope_class_label.str2int(sample) for sample in labels]\n",
    "    }, \n",
    "    input_columns=[\"sequence\", \"paratope_labels\"], batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': Value(dtype='string', id=None),\n",
       " 'paratope_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the old Features instance from the previous ab_dataset\n",
    "# Notice how labels is a Sequence of Value\n",
    "feature_set_copy = ab_dataset['train'].features.copy()\n",
    "feature_set_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast to the `new_feature` that we made earlier\n",
    "feature_set_copy['paratope_labels'] = new_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0875aa0154b4c2f8597cfb840d39b31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095a4a7e8c9d4137b84685e508fd8250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3391feea0f1d4c96a3286eec48e0b6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ab_dataset_featurised = ab_dataset_featurised.cast(feature_set_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': Value(dtype='string', id=None),\n",
       " 'paratope_labels': Sequence(feature=ClassLabel(names=['N', 'P'], id=None), length=-1, id=None),\n",
       " '__index_level_0__': Value(dtype='int64', id=None)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab_dataset_featurised['train'].features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# now the labels are actually a series of integers, but is recognised by huggingface as a series of Classlabels\n",
    "print(ab_dataset_featurised['train'].select(range(1))['paratope_labels'][0][20:35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to redefine this to e.g. put -100 labels for the start/end tokens\n",
    "\n",
    "def preprocess(batch):\n",
    "    # :facepalm: The preprocess function takes tokenizer and needs a LIST not a PT tensor :eyeroll:\n",
    "    # https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb#scrollTo=vc0BSBLIIrJQ\n",
    "    \n",
    "    t_inputs = tokenizer(batch['sequence'], \n",
    "        padding=\"max_length\")\n",
    "    batch['input_ids'] = t_inputs.input_ids\n",
    "    batch['attention_mask'] = t_inputs.attention_mask\n",
    "    \n",
    "    # enumerate \n",
    "    labels_container = []\n",
    "    for index, labels in enumerate(batch['paratope_labels']):\n",
    "        \n",
    "        # This is typically length of the sequence + SOS + EOS + PAD (to longest example in batch)\n",
    "        tokenized_input_length = len(batch['input_ids'][index])\n",
    "        paratope_label_length  = len(batch['paratope_labels'][index])\n",
    "        \n",
    "        # we subtract 1 because we start with SOS\n",
    "        # we should in theory have at least 1 \"pad_with_eos\" because an EOS wouldn't have been accounted for in the\n",
    "        # paratope_labels column even for the longest possible sequence\n",
    "        n_pads_with_eos = max(1, tokenized_input_length - paratope_label_length - 1)\n",
    "        \n",
    "        # We have a starting -100 for the SOS\n",
    "        # and fill the rest of seq length with -100 to account for any extra pads and the final EOS token\n",
    "        # The -100s are ignored in the CE loss function\n",
    "        labels_padded = [-100] + labels + [-100] * n_pads_with_eos\n",
    "        \n",
    "        assert len(labels_padded) == len(batch['input_ids'][index]), \\\n",
    "        f\"Lengths don't align, {len(labels_padded)}, {len(batch['input_ids'][index])}, {len(labels)}\"\n",
    "        \n",
    "        labels_container.append(labels_padded)\n",
    "    \n",
    "    # We create a new column called `labels`, which is recognised by the HF trainer object\n",
    "    batch['labels'] = labels_container\n",
    "    \n",
    "    for i,v in enumerate(batch['labels']):\n",
    "        assert len(batch['input_ids'][i]) == len(v) == len(batch['attention_mask'][i])\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a108d0f4fbc741458c84d0012c0eaffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/90 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c90288378ca425b94fd5ad47b878505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af8c4f20fec4b1abf038b13b8b89ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply that function above on the dataset - labels now aligned!\n",
    "ab_dataset_tokenized = ab_dataset_featurised.map(\n",
    "    preprocess, \n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    remove_columns=['sequence', 'paratope_labels']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model set-up and training\n",
    "\n",
    "Here we define:\n",
    "* The callback function to compute some metrics during training (and can be used for evaluation!)\n",
    "* The training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This has the actual names that maps 0->N and 1->P\n",
    "label_list = paratope_class_label.names\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \"\"\"\n",
    "    A callback added to the trainer so that we calculate various metrics via sklearn\n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    \n",
    "    # The predictions are logits, so we apply softmax to get the probabilities. We only need\n",
    "    # the probabilities of the paratope label, which is index 1 (according to the ClassLabel we made earlier),\n",
    "    # or the last column from the output tensor\n",
    "    prediction_pr = torch.softmax(torch.from_numpy(predictions), dim=2).detach().numpy()[:,:,-1]\n",
    "    \n",
    "    # We run an argmax to get the label\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Only compute on positions that are not labelled -100\n",
    "    preds = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    labs = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    probs = [ \n",
    "        [prediction_pr[i][pos] for (pr, (pos, l)) in zip(prediction, enumerate(label)) if l!=-100]\n",
    "         for i, (prediction, label) in enumerate(zip(predictions, labels)) \n",
    "    ] \n",
    "            \n",
    "    # flatten\n",
    "    preds = sum(preds, [])\n",
    "    labs = sum(labs, [])\n",
    "    probs = sum(probs,[])\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision_score(labs, preds, pos_label=\"P\"),\n",
    "        \"recall\": recall_score(labs, preds, pos_label=\"P\"),\n",
    "        \"f1\": f1_score(labs, preds, pos_label=\"P\"),\n",
    "        \"auc\": roc_auc_score(labs, probs),\n",
    "        \"aupr\": average_precision_score(labs, probs, pos_label=\"P\"),\n",
    "        \"mcc\": matthews_corrcoef(labs, preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size, metric you want etc. \n",
    "batch_size = 32\n",
    "RUN_ID = \"paratope-prediction-task\"\n",
    "SEED = 0\n",
    "LR = 1e-6\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{RUN_ID}_{SEED}\", # this is the name of the checkpoint folder\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=LR, # 1e-6, 5e-6, 1e-5. .... 1e-3\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=10,\n",
    "    warmup_ratio=0, # 0, 0.05, 0.1 .... \n",
    "    load_best_model_at_end=True,\n",
    "    lr_scheduler_type='linear',\n",
    "    metric_for_best_model='aupr', # name of the metric here should correspond to metrics defined in compute_metrics\n",
    "    logging_strategy='epoch',\n",
    "    seed=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set all seeds to make results reproducible (deterministic mode).\n",
    "    When seed is None, disables deterministic mode.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of '../antiberta/antiberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '../antiberta/antiberta-base' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/configuration_utils.py:614\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    615\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    616\u001b[0m         configuration_file,\n\u001b[1;32m    617\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    618\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    619\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    620\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    621\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    622\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    623\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    624\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    625\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    626\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    627\u001b[0m     )\n\u001b[1;32m    628\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    410\u001b[0m         path_or_repo_id,\n\u001b[1;32m    411\u001b[0m         filename,\n\u001b[1;32m    412\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    413\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    414\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    415\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    416\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    417\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    418\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    419\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    420\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    421\u001b[0m     )\n\u001b[1;32m    423\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrepo_id\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 114\u001b[0m     validate_repo_id(arg_value)\n\u001b[1;32m    116\u001b[0m \u001b[39melif\u001b[39;00m arg_name \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtoken\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m arg_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:166\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m repo_id\u001b[39m.\u001b[39mcount(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    167\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRepo id must be in the form \u001b[39m\u001b[39m'\u001b[39m\u001b[39mrepo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnamespace/repo_name\u001b[39m\u001b[39m'\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mrepo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. Use `repo_type` argument if needed.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m     )\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m REPO_ID_REGEX\u001b[39m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../antiberta/antiberta-base'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m MODEL_DIR \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../antiberta/antiberta-base\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39m# We initialise a model using the weights from the pre-trained model\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m RobertaForTokenClassification\u001b[39m.\u001b[39;49mfrom_pretrained(MODEL_DIR, num_labels\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     10\u001b[0m     model,\n\u001b[1;32m     11\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics\n\u001b[1;32m     16\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/modeling_utils.py:1966\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1964\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   1965\u001b[0m     config_path \u001b[39m=\u001b[39m config \u001b[39mif\u001b[39;00m config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m pretrained_model_name_or_path\n\u001b[0;32m-> 1966\u001b[0m     config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mconfig_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m   1967\u001b[0m         config_path,\n\u001b[1;32m   1968\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m   1969\u001b[0m         return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1970\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m   1971\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m   1972\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1973\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m   1974\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1975\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m   1976\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m   1977\u001b[0m         _from_auto\u001b[39m=\u001b[39;49mfrom_auto_class,\n\u001b[1;32m   1978\u001b[0m         _from_pipeline\u001b[39m=\u001b[39;49mfrom_pipeline,\n\u001b[1;32m   1979\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1980\u001b[0m     )\n\u001b[1;32m   1981\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1982\u001b[0m     model_kwargs \u001b[39m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/configuration_utils.py:532\u001b[0m, in \u001b[0;36mPretrainedConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    455\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_pretrained\u001b[39m(\u001b[39mcls\u001b[39m, pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike], \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mPretrainedConfig\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    457\u001b[0m \u001b[39m    Instantiate a [`PretrainedConfig`] (or a derived class) from a pretrained model configuration.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39m    assert unused_kwargs == {\"foo\": False}\u001b[39;00m\n\u001b[1;32m    531\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 532\u001b[0m     config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    533\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type:\n\u001b[1;32m    534\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    535\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mYou are using a model of type \u001b[39m\u001b[39m{\u001b[39;00mconfig_dict[\u001b[39m'\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m to instantiate a model of type \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mmodel_type\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported for all configurations of models and can yield errors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/configuration_utils.py:559\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    558\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    561\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/configuration_utils.py:635\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[39mraise\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m         \u001b[39m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt load the configuration of \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. If you were trying to load it\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m from \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, make sure you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt have a local directory with the same\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m name. Otherwise, make sure \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mpretrained_model_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is the correct path to a directory\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m containing a \u001b[39m\u001b[39m{\u001b[39;00mconfiguration_file\u001b[39m}\u001b[39;00m\u001b[39m file\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    640\u001b[0m         )\n\u001b[1;32m    642\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    643\u001b[0m     \u001b[39m# Load config dict\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     config_dict \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load the configuration of '../antiberta/antiberta-base'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '../antiberta/antiberta-base' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "# Name of the pre-trained model after you train your MLM\n",
    "MODEL_DIR = \"antiberta-base\"\n",
    "\n",
    "# We initialise a model using the weights from the pre-trained model\n",
    "model = RobertaForTokenClassification.from_pretrained(MODEL_DIR, num_labels=2)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ab_dataset_tokenized['train'],\n",
    "    eval_dataset=ab_dataset_tokenized['validation'],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# watch stuff fly\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run prediction on the test set\n",
    "pred = trainer.predict(\n",
    "    ab_dataset_tokenized['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this stores a JSON with metric values\n",
    "pred.metrics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference - how to go from sequence to predicted sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input sequence of tralokinumab Light chain\n",
    "input_seq = 'YVLTQPPSVSVAPGKTARITCGGNIIGSKLVHWYQQKPGQAPVLVIYDDGDRPSGIPERFSGSNSGNTATLTISRVEAGDEADYYCQVWDTGSDPVVFGGGTKLTVL'\n",
    "model = RobertaForTokenClassification.from_pretrained(\n",
    "    f\"{RUN_ID}_{SEED}\"\n",
    ")\n",
    "\n",
    "tokenized_input = tokenizer([input_seq], return_tensors='pt', padding=True)\n",
    "predicted_logits = model(**tokenized_input)\n",
    "\n",
    "# Simple argmax - no thresholding.\n",
    "argmax = predicted_logits[0].argmax(2)[0][1:-1].cpu().numpy()\n",
    "indices = np.argwhere(argmax).flatten()\n",
    "\n",
    "predicted_sequence = ''\n",
    "\n",
    "for i, s in enumerate(input_seq):\n",
    "    if i in indices:\n",
    "        predicted_sequence += s\n",
    "    else:\n",
    "        predicted_sequence += '-'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c38c8779811d1cfaf4b5a784c97578f212c26cffc36ab1ef679f872ba1fdba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
