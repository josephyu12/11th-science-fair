{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked Language Modeling\n",
    "\n",
    "This notebook describes how one can pre-train their own AntiBERTa model using the HuggingFace framework. As a demo, we've included the tokenizer we've used, and 1% of the sequences that we used in our training, validation, and test sets of the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of all the things we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some imports \n",
    "from transformers import (\n",
    "    RobertaConfig,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# Initialise the tokeniser\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\n",
    "    \"../antiberta/antibody-tokenizer\"\n",
    ")\n",
    "\n",
    "# Initialise the data collator, which is necessary for batching\n",
    "collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4099dca2205c4257\n",
      "Found cached dataset text (/Users/joseph/.cache/huggingface/datasets/text/default-4099dca2205c4257/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ddc0d6f84e843cdb24ff851a228f9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/joseph/.cache/huggingface/datasets/text/default-4099dca2205c4257/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-5c6d18c0ea3c8c1f.arrow\n",
      "Loading cached processed dataset at /Users/joseph/.cache/huggingface/datasets/text/default-4099dca2205c4257/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-85b0fa99574ce78c.arrow\n",
      "Loading cached processed dataset at /Users/joseph/.cache/huggingface/datasets/text/default-4099dca2205c4257/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-42f591da6c4e4415.arrow\n"
     ]
    }
   ],
   "source": [
    "text_datasets = {\n",
    "    \"train\": ['../antiberta/assets/train-slice.txt'],\n",
    "    \"eval\": ['../antiberta/assets/val-slice.txt'],\n",
    "    \"test\": ['../antiberta/assets/test-slice.txt']\n",
    "}\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=text_datasets)\n",
    "tokenized_dataset = dataset.map(\n",
    "    lambda z: tokenizer(\n",
    "        z[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=150,\n",
    "        return_special_tokens_mask=True,\n",
    "    ),\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the cofigurations we've used for pre-training.\n",
    "antiberta_config = {\n",
    "    \"num_hidden_layers\": 12,\n",
    "    \"num_attention_heads\": 12,\n",
    "    \"hidden_size\": 768,\n",
    "    \"d_ff\": 3072,\n",
    "    \"vocab_size\": 25,\n",
    "    \"max_len\": 150,\n",
    "    \"max_position_embeddings\": 152,\n",
    "    \"batch_size\": 96,\n",
    "    \"max_steps\": 225000,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"peak_learning_rate\": 0.0001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the model\n",
    "model_config = RobertaConfig(\n",
    "    vocab_size=antiberta_config.get(\"vocab_size\"),\n",
    "    hidden_size=antiberta_config.get(\"hidden_size\"),\n",
    "    max_position_embeddings=antiberta_config.get(\"max_position_embeddings\"),\n",
    "    num_hidden_layers=antiberta_config.get(\"num_hidden_layers\", 12),\n",
    "    num_attention_heads=antiberta_config.get(\"num_attention_heads\", 12),\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "model = RobertaForMaskedLM(model_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct training arguments\n",
    "# Huggingface uses a default seed of 42\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"test\",\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=antiberta_config.get(\"batch_size\", 32),\n",
    "    per_device_eval_batch_size=antiberta_config.get(\"batch_size\", 32),\n",
    "    max_steps=225000,\n",
    "    save_steps=2500,\n",
    "    logging_steps=2500,\n",
    "    adam_beta2=0.98,\n",
    "    adam_epsilon=1e-6,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=10000,\n",
    "    learning_rate=1e-4,\n",
    "    gradient_accumulation_steps=antiberta_config.get(\"gradient_accumulation_steps\", 1),\n",
    "    # fp16=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup of the HuggingFace Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    data_collator=collator,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"eval\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from test/checkpoint-215000.\n",
      "The following columns in the training set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3\n",
      "  Num Epochs = 225000\n",
      "  Instantaneous batch size per device = 96\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 96\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 225000\n",
      "  Number of trainable parameters = 85784857\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 215000\n",
      "  Continuing training from global step 215000\n",
      "  Will skip the first 215000 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f609f8aa714e318e19592c72b09ded",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58404d3eef04c83bdd6911b62df743e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/225000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 3.488372093023256e-06, 'epoch': 217500.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2618df659af400ea76fc93fd0aa950c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test/checkpoint-217500\n",
      "Configuration saved in test/checkpoint-217500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.2258, 'eval_samples_per_second': 13.285, 'eval_steps_per_second': 4.428, 'epoch': 217500.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test/checkpoint-217500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 2.325581395348837e-06, 'epoch': 220000.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d96b0918dbb4f478a1bed68b943c303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test/checkpoint-220000\n",
      "Configuration saved in test/checkpoint-220000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.2666, 'eval_samples_per_second': 11.251, 'eval_steps_per_second': 3.75, 'epoch': 220000.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test/checkpoint-220000/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 1.1627906976744186e-06, 'epoch': 222500.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6407f20b55784ae089013cc23c23970c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test/checkpoint-222500\n",
      "Configuration saved in test/checkpoint-222500/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.2304, 'eval_samples_per_second': 13.02, 'eval_steps_per_second': 4.34, 'epoch': 222500.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test/checkpoint-222500/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3\n",
      "  Batch size = 96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'learning_rate': 0.0, 'epoch': 225000.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5915492de1f48159a58da96b53dc143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to test/checkpoint-225000\n",
      "Configuration saved in test/checkpoint-225000/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': nan, 'eval_runtime': 0.2206, 'eval_samples_per_second': 13.596, 'eval_steps_per_second': 4.532, 'epoch': 225000.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in test/checkpoint-225000/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 7336.9904, 'train_samples_per_second': 2943.986, 'train_steps_per_second': 30.667, 'train_loss': 0.0, 'epoch': 225000.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=225000, training_loss=0.0, metrics={'train_runtime': 7336.9904, 'train_samples_per_second': 2943.986, 'train_steps_per_second': 30.667, 'train_loss': 0.0, 'epoch': 225000.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../antiberta/saved_model\n",
      "Configuration saved in ../antiberta/saved_model/config.json\n",
      "Model weights saved in ../antiberta/saved_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('../antiberta/saved_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3\n",
      "  Batch size = 96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c826a5f8a7ed498f9fe5fe8309edbd14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict MLM performance on the test dataset\n",
    "out = trainer.predict(tokenized_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[[-0.44890258,  0.        ,  0.39918557, ..., -0.67570436,\n",
      "         -0.19572093, -0.6968133 ],\n",
      "        [-0.21400794,  0.        ,  0.10926522, ..., -1.2135879 ,\n",
      "          0.14994998, -0.8182367 ],\n",
      "        [-0.2807524 ,  0.        ,  0.37963933, ..., -1.0091581 ,\n",
      "          0.21263681, -0.83665437],\n",
      "        ...,\n",
      "        [-0.5694531 ,  0.        ,  0.06545924, ..., -0.8455439 ,\n",
      "          0.14736086, -0.55978304],\n",
      "        [-0.5694531 ,  0.        ,  0.06545924, ..., -0.8455439 ,\n",
      "          0.14736086, -0.55978304],\n",
      "        [-0.5694531 ,  0.        ,  0.06545924, ..., -0.8455439 ,\n",
      "          0.14736086, -0.55978304]],\n",
      "\n",
      "       [[-0.47246504,  0.        ,  0.4012822 , ..., -0.6577634 ,\n",
      "         -0.21133558, -0.71489227],\n",
      "        [-0.24963681,  0.        ,  0.10322255, ..., -1.1950003 ,\n",
      "          0.12808968, -0.84429336],\n",
      "        [-0.31693763,  0.        ,  0.39187625, ..., -0.9945083 ,\n",
      "          0.17764995, -0.8599656 ],\n",
      "        ...,\n",
      "        [-0.59228003,  0.        ,  0.07220931, ..., -0.8139091 ,\n",
      "          0.12768194, -0.57575554],\n",
      "        [-0.59228003,  0.        ,  0.07220931, ..., -0.8139091 ,\n",
      "          0.12768194, -0.57575554],\n",
      "        [-0.59228003,  0.        ,  0.07220931, ..., -0.8139091 ,\n",
      "          0.12768194, -0.57575554]],\n",
      "\n",
      "       [[-0.4491232 ,  0.        ,  0.35148138, ..., -0.7377599 ,\n",
      "         -0.11701317, -0.71854967],\n",
      "        [-0.26470935,  0.        ,  0.08748181, ..., -1.311495  ,\n",
      "          0.22347769, -0.80149925],\n",
      "        [-0.30643246,  0.        ,  0.32790053, ..., -1.1006247 ,\n",
      "          0.28307953, -0.8268136 ],\n",
      "        ...,\n",
      "        [-0.5798853 ,  0.        ,  0.021609  , ..., -0.9144991 ,\n",
      "          0.2069519 , -0.58235914],\n",
      "        [-0.5798853 ,  0.        ,  0.021609  , ..., -0.9144991 ,\n",
      "          0.2069519 , -0.58235914],\n",
      "        [-0.5798853 ,  0.        ,  0.021609  , ..., -0.9144991 ,\n",
      "          0.2069519 , -0.58235914]]], dtype=float32), label_ids=array([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100],\n",
      "       [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100],\n",
      "       [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100]]), metrics={'test_loss': nan, 'test_runtime': 0.1971, 'test_samples_per_second': 15.222, 'test_steps_per_second': 5.074})\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c38c8779811d1cfaf4b5a784c97578f212c26cffc36ab1ef679f872ba1fdba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
