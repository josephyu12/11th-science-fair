{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1dcb7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c6abb663794789ae227c035151f2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bdc7ad-1599-4ccd-8abc-5536833571b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from Bio.Seq import Seq\n",
    "from transformers import T5Tokenizer, TFT5EncoderModel\n",
    "import re\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "import pickle\n",
    "import sys\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0749e1",
   "metadata": {
    "id": "5c0749e1"
   },
   "source": [
    "# Fine-Tuning Protein Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81db83",
   "metadata": {
    "id": "1d81db83"
   },
   "source": [
    "In this notebook, we're going to do some transfer learning to fine-tune some large, pre-trained protein language models on tasks of interest. If that sentence feels a bit intimidating to you, don't panic - there's a blog post coming soon that explains the concepts here in much more detail.\n",
    "\n",
    "The specific model we're going to use is ESM-2, which is the state-of-the-art protein language model at the time of writing (November 2022). The citation for this model is [Lin et al, 2022](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).\n",
    "\n",
    "There are several ESM-2 checkpoints with differing model sizes. Larger models will generally have better accuracy, but they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints (at time of writing) are:\n",
    "\n",
    "| Checkpoint name | Num layers | Num parameters |\n",
    "|------------------------------|----|----------|\n",
    "| `esm2_t48_15B_UR50D`         | 48 | 15B     |\n",
    "| `esm2_t36_3B_UR50D`          | 36 | 3B      | \n",
    "| `esm2_t33_650M_UR50D`        | 33 | 650M    | \n",
    "| `esm2_t30_150M_UR50D`        | 30 | 150M    | \n",
    "| `esm2_t12_35M_UR50D`         | 12 | 35M     | \n",
    "| `esm2_t6_8M_UR50D`           | 6  | 8M      | \n",
    "\n",
    "Note that the larger checkpoints may be very difficult to train without a large cloud GPU like an A100 or H100, and the largest 15B parameter checkpoint will probably be impossible to train on **any** single GPU! Also, note that memory usage for attention during training will scale as `O(batch_size * num_layers * seq_len^2)`, so larger models on long sequences will use quite a lot of memory! We will use the `esm2_t12_35M_UR50D` checkpoint for this notebook, which should train on any Colab instance or modern GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e605a2",
   "metadata": {
    "id": "32e605a2"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"../antiberta/saved_model/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6ac19",
   "metadata": {
    "id": "a8e6ac19"
   },
   "source": [
    "# Sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb400c",
   "metadata": {
    "id": "c3eb400c"
   },
   "source": [
    "One of the most common tasks you can perform with a language model is **sequence classification**. In sequence classification, we classify an entire protein into a category, from a list of two or more possibilities. There's no limit on the number of categories you can use, or the specific problem you choose, as long as it's something the model could in theory infer from the raw protein sequence. To keep things simple for this example, though, let's try classifying proteins by their cellular localization - given their sequence, can we predict if they're going to be found in the cytosol (the fluid inside the cell) or embedded in the cell membrane?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc122f",
   "metadata": {
    "id": "c5bc122f"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91d394",
   "metadata": {
    "id": "4c91d394"
   },
   "source": [
    "In this section, we're going to gather some training data from UniProt. Our goal is to create a pair of lists: `sequences` and `labels`. `sequences` will be a list of protein sequences, which will just be strings like \"MNKL...\", where each letter represents a single amino acid in the complete protein. `labels` will be a list of the category for each sequence. The categories will just be integers, with 0 representing the first category, 1 representing the second and so on. In other words, if `sequences[i]` is a protein sequence then `labels[i]` should be its corresponding category. These will form the **training data** we're going to use to teach the model the task we want it to do.\n",
    "\n",
    "If you're adapting this notebook for your own use, this will probably be the main section you want to change! You can do whatever you want here, as long as you create those two lists by the end of it. If you want to follow along with this example, though, first we'll need to `import requests` and set up our query to UniProt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550d85f1-6320-432d-8b20-b6531a0fc845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VHorVHH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QITLKESGPTLVKPTQTLTLTCKLSGFSVNTGGVGVGWIRQPPGKA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGDTFNIYAINWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFNSYAITWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>QVQLVESGGGVVQPGRSLRLSCAASGFTFSTHGMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFRRYAISWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11862</th>\n",
       "      <td>EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11863</th>\n",
       "      <td>QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11864</th>\n",
       "      <td>QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11865</th>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11866</th>\n",
       "      <td>QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11415 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 VHorVHH\n",
       "5      QITLKESGPTLVKPTQTLTLTCKLSGFSVNTGGVGVGWIRQPPGKA...\n",
       "32     QVQLVQSGAEVKKPGSSVKVSCKASGDTFNIYAINWVRQAPGQGLE...\n",
       "33     QVQLVQSGAEVKKPGSSVKVSCKASGGTFNSYAITWVRQAPGQGLE...\n",
       "34     QVQLVESGGGVVQPGRSLRLSCAASGFTFSTHGMHWVRQAPGKGLE...\n",
       "35     QVQLVQSGAEVKKPGSSVKVSCKASGGTFRRYAISWVRQAPGQGLE...\n",
       "...                                                  ...\n",
       "11862  EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...\n",
       "11863  QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...\n",
       "11864  QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...\n",
       "11865  EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...\n",
       "11866  QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...\n",
       "\n",
       "[11415 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/CoV-AbDab_031022.csv\")\n",
    "df = df[[\"VHorVHH\"]]\n",
    "df = df[df[\"VHorVHH\"].apply(lambda x: len(x) <= 138)]\n",
    "df = df[(df.VHorVHH != 'ND')]\n",
    "df\n",
    "# df = df[[\"CDRH3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca827dc2-3d2e-4bbd-80ae-08b1ea26e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/Bio/Seq.py:3482: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVQLVQSGPEVKKPGSSVKVSCKASGGTFSNFAFSWVRQAPGQGLEWMGSVILHLGTSTYAQKFQGRVTITADESTSAAFMDLNALTSDDTAVYYCARVVAVPGRVPYWFDPWGQGTLVTVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARVPPTSTVTTLGDDYWGQGTLVTVSS', 'QVQLVQSGPEVKKPGASVRVSCKPSGYPFSNYGISWMRQAPGQGLEWMGWVNIDKGNTKYAQKFQDRVTMTTDTSSSTVYLELRSLRSDDTALYYCARERGGYRYGDYWGQGTLVIVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEIKHSGSTNYIPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCASRAGAAAASWGQGTLVTVSS', 'SETLSLTCAVHGGSFSDYYWTWIRQPPGKGLEWIGEINHRGGTNYNPSLKSRLNILVDTSKSQFSLKLSSVTAADTAVYFCARERFILIRGLTKYYYYMDVWGKGTTVTVS'] 11415\n"
     ]
    }
   ],
   "source": [
    "dummy = []\n",
    "head = []\n",
    "with open(\"../Data/cAb-rep/cAb-Rep_heavy.nt.txt\") as myfile:\n",
    "    # count = 0\n",
    "    for i in myfile:\n",
    "        # if count <= 1:\n",
    "        #     print(i)\n",
    "        #     if i.find(\">\") == -1 & i.find(\"-\") == -1:\n",
    "        #         print(Seq.translate(i.strip()))\n",
    "        #     count+=1\n",
    "        dummy.append(i)\n",
    "    np.random.shuffle(dummy)\n",
    "    \n",
    "    for i in dummy:\n",
    "        if i.find(\">\") == -1 & i.find(\"-\") == -1 & i.find(\"N\") == -1: # These conditions must be met for a valid sequence, the longest was 141. However, there is no 141 sequence for COVID, the greatest is 138, so we go with that\n",
    "            aa_sequence = Seq.translate(i.strip())\n",
    "            if (len(aa_sequence) <= 138) & (len(aa_sequence) >= 100):\n",
    "                head.append(aa_sequence)\n",
    "                if len(head) >= 11415:\n",
    "                    break\n",
    "print(head[:5], len(head))\n",
    "healthy_sequences = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3c0779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del head\n",
    "del myfile\n",
    "del dummy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67483afe-f027-4dd9-9d03-e62e4145b446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "covid_sequences = df.to_numpy()\n",
    "covid_sequences = np.squeeze(covid_sequences)\n",
    "np.random.shuffle(covid_sequences)\n",
    "print(len(max(healthy_sequences, key=len)))\n",
    "print(len(max(covid_sequences, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42d88a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b745d707-e2ff-4562-9e2d-a602b198b1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "healthy_lables = [0] * 11415\n",
    "covid_lables = [1] * 11415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e4d82be-b95e-4fa8-b69b-8595404f4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((healthy_sequences, covid_sequences))\n",
    "y = np.concatenate((healthy_lables, covid_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1335e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.tolist()\n",
    "y = y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "568d6d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del healthy_sequences\n",
    "del covid_sequences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aac39c",
   "metadata": {
    "id": "e0aac39c"
   },
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9099e7c",
   "metadata": {
    "id": "a9099e7c"
   },
   "source": [
    "Since the data we're loading isn't prepared for us as a machine learning dataset, we'll have to split the data into train and test sets ourselves! We can use sklearn's function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7452eec8-9c73-452a-b112-92a653d1fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22372ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29b4ed",
   "metadata": {
    "id": "7d29b4ed"
   },
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02baaf7",
   "metadata": {
    "id": "c02baaf7"
   },
   "source": [
    "All inputs to neural nets must be numerical. The process of converting strings into numerical indices suitable for a neural net is called **tokenization**. For natural language this can be quite complex, as usually the network's vocabulary will not contain every possible word, which means the tokenizer must handle splitting rarer words into pieces, as well as all the complexities of capitalization and unicode characters and so on.\n",
    "\n",
    "With proteins, however, things are very easy. In protein language models, each amino acid is converted to a single token. Every model on `transformers` comes with an associated `tokenizer` that handles tokenization for it, and protein language models are no different. Let's get our tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddbe2b2d",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b66e9b0e44f34a59bc6e29961cd2b418",
      "fdfcf0f1f6ba4ef1b40691b7d51217cc",
      "0c669f05694948dc83b6a4efd112adc0"
     ]
    },
    "id": "ddbe2b2d",
    "outputId": "99c0510f-00cd-47c4-ad92-18bc024d822c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'RobertaTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"../antiberta/antibody-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16be37",
   "metadata": {
    "id": "9d16be37"
   },
   "source": [
    "Let's try a single sequence to see what the outputs from our tokenizer look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "687386af",
   "metadata": {
    "id": "687386af",
    "outputId": "aa4a3278-b638-4b87-c55c-0beaf8fa184b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 8, 22, 18, 14, 22, 8, 20, 10, 10, 10, 14, 12, 18, 17, 10, 10, 20, 14, 19, 14, 20, 6, 5, 5, 20, 10, 9, 21, 22, 20, 20, 16, 24, 15, 20, 23, 22, 19, 18, 5, 17, 10, 13, 10, 14, 8, 23, 22, 20, 22, 12, 24, 20, 10, 10, 20, 21, 24, 24, 5, 7, 20, 22, 13, 10, 19, 9, 21, 22, 20, 19, 7, 16, 20, 13, 16, 21, 14, 24, 14, 18, 15, 16, 20, 14, 19, 5, 8, 7, 21, 5, 22, 24, 24, 6, 5, 19, 10, 10, 19, 24, 7, 24, 7, 22, 9, 7, 12, 23, 10, 18, 10, 21, 15, 22, 21, 22, 20, 20, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a719808",
   "metadata": {
    "id": "9a719808"
   },
   "source": [
    "This looks good! We can see that our sequence has been converted into `input_ids`, which is the tokenized sequence, and an `attention_mask`. The attention mask handles the case when we have sequences of variable length - in those cases, the shorter sequences are padded with blank \"padding\" tokens, and the attention mask is padded with 0s to indicate that those tokens should be ignored by the model.\n",
    "\n",
    "So now, let's tokenize our whole dataset. Note that we don't need to do anything with the labels, as they're already in the format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56e26ddf",
   "metadata": {
    "id": "56e26ddf"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(X_train)\n",
    "test_tokenized = tokenizer(X_test)\n",
    "val_tokenized = tokenizer(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3681d1",
   "metadata": {
    "id": "df3681d1"
   },
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85089e49",
   "metadata": {
    "id": "85089e49"
   },
   "source": [
    "Now we want to turn this data into a dataset that Keras can load samples from. We can use the HuggingFace `Dataset` class for this, which has convenience functions to wrap itself with a `tf.data.Dataset`, although there are a number of different approaches that you can take at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb79ba6c",
   "metadata": {
    "id": "fb79ba6c",
    "outputId": "83cea580-6d6b-4efe-d819-6c44da023763"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 18492\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "val_dataset = Dataset.from_dict(val_tokenized)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e809e47",
   "metadata": {
    "id": "9e809e47"
   },
   "source": [
    "This looks good, but we're missing our labels! Let's add those on as an extra column to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "090acc0d",
   "metadata": {
    "id": "090acc0d",
    "outputId": "bf732b0d-e9c1-44b6-9e14-37f099c29ea9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 18492\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", y_train) # train_labels = y_train\n",
    "test_dataset = test_dataset.add_column(\"labels\", y_test)\n",
    "val_dataset = val_dataset.add_column(\"labels\", y_val)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9aaa8",
   "metadata": {
    "id": "ced9aaa8"
   },
   "source": [
    "Looks good! We're ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af074a5c",
   "metadata": {
    "id": "af074a5c"
   },
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab5d70",
   "metadata": {
    "id": "ccab5d70"
   },
   "source": [
    "Next, we want to load our model. Make sure to use exactly the same model as you used when loading the tokenizer, or your model might not understand the tokenization scheme you're using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc164b49",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "365446cb74e243a8b5555c688f6b2c20"
     ]
    },
    "id": "fc164b49",
    "outputId": "f1f0781d-04f9-4fab-9864-b67d1a666707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels: 2\n",
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n",
      "maxCacheSize: 24.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 10:47:07.743581: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-02 10:47:07.743721: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForSequenceClassification: ['roberta.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFRobertaForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "num_labels = max(y_train + y_test) + 1  # Add 1 since 0 can be a label\n",
    "print(\"Num labels:\", num_labels)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels, from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcba23",
   "metadata": {
    "id": "49dcba23"
   },
   "source": [
    "These warnings are telling us that the model is discarding some weights that it used for language modelling (the `lm_head`) and adding some weights for sequence classification (the `classifier`). This is exactly what we expect when we want to fine-tune a language model on a sequence classification task!\n",
    "\n",
    "Next, let's prepare our `tf.data.Dataset`. This Dataset will stream samples from our Huggingface `Dataset` in a way that Keras natively understands - once we've created it, we can pass it straight to `model.fit()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14fcf32d",
   "metadata": {
    "id": "14fcf32d"
   },
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_val_set = model.prepare_tf_dataset(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95d099",
   "metadata": {
    "id": "bc95d099"
   },
   "source": [
    "You might wonder why we pass along the `tokenizer` when we already preprocessed our data. This is because we will use it one last time to make all the samples we gather the same length by applying padding, which requires knowing the model's preferences regarding padding (to the left or right? with which token?). The `tokenizer` has a `pad()` method that will do all of this right for us, and `prepare_tf_dataset` will use it.\n",
    "\n",
    "Now all we need to do is compile our model. We use the `AdamWeightDecay` optimizer, which usually performs a little better than the base `Adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d36462cb",
   "metadata": {
    "id": "d36462cb",
    "outputId": "69d20080-4b34-453b-90ea-f6effab767ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " roberta (TFRobertaMainLayer  multiple                 85192704  \n",
      " )                                                               \n",
      "                                                                 \n",
      " classifier (TFRobertaClassi  multiple                 592130    \n",
      " ficationHead)                                                   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85,784,834\n",
      "Trainable params: 85,784,834\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "\n",
    "model.compile(optimizer=AdamWeightDecay(2e-5), metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dcf25",
   "metadata": {
    "id": "709dcf25"
   },
   "source": [
    "And now we can fit our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e212b751",
   "metadata": {
    "id": "e212b751",
    "outputId": "29146681-2362-4a54-a6f2-cfde6f87c406",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 10:47:15.963122: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-02 10:47:24.353899: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2311/2311 [==============================] - ETA: 0s - loss: 0.5361 - accuracy: 0.6992"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 11:23:17.873295: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2311/2311 [==============================] - 2401s 1s/step - loss: 0.5361 - accuracy: 0.6992 - val_loss: 0.4661 - val_accuracy: 0.7416\n",
      "Epoch 2/3\n",
      "2311/2311 [==============================] - 3265s 1s/step - loss: 0.4552 - accuracy: 0.7608 - val_loss: 0.4510 - val_accuracy: 0.7781\n",
      "Epoch 3/3\n",
      "2311/2311 [==============================] - 5409s 2s/step - loss: 0.4363 - accuracy: 0.7741 - val_loss: 0.4255 - val_accuracy: 0.7864\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e2307820>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_set, validation_data=tf_val_set, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec59f4",
   "metadata": {
    "id": "dfec59f4"
   },
   "source": [
    "Nice! After three epochs we have a model accuracy of ~94%. Note that we didn't do a lot of work to filter the training data or tune hyperparameters for this experiment, and also that we used one of the smallest ESM-2 models. With a larger starting model and more effort to ensure that the training data categories were cleanly separable, accuracy could almost certainly go a lot higher!\n",
    "\n",
    "Now that we're done, let's see how we can upload our model to the HuggingFace Hub. This step is optional, but will allow us to easily share it with other researchers. If you encounter any errors here, make sure you ran the login cell at the top of the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2214f8",
   "metadata": {
    "id": "2e2214f8"
   },
   "source": [
    "First, let's set a couple of properties on our model. This is optional, but will ensure the model knows the names of its categories, rather than just outputting \"0\" or \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4ca836e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.label2id = {\"healthy\": 0, \"covid\": 1}\n",
    "model.id2label = {val: key for key, val in model.label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868bf45",
   "metadata": {
    "id": "8868bf45"
   },
   "source": [
    "Now we can push it to the hub as simply as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "770c5bb1",
   "metadata": {
    "id": "770c5bb1",
    "outputId": "5eebae66-43c8-4d6e-c51f-0fd64ab8fe5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/josephyu12/antiberta-finetuned-healthy-covid-classification/commit/ae12279ef9f95e9633490fca1bf74ead07052971', commit_message='Upload tokenizer', commit_description='', oid='ae12279ef9f95e9633490fca1bf74ead07052971', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"antiberta\"\n",
    "finetuned_model_name = f\"{model_name}-finetuned-healthy-covid-classification\"\n",
    "\n",
    "model.push_to_hub(finetuned_model_name)\n",
    "tokenizer.push_to_hub(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc1993",
   "metadata": {
    "id": "00fc1993"
   },
   "source": [
    "If you used the code above, you can now share this model with all your friends, family or favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"your-username/my-awesome-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1842d9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c38c8779811d1cfaf4b5a784c97578f212c26cffc36ab1ef679f872ba1fdba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
