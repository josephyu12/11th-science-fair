{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1dcb7fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c6abb663794789ae227c035151f2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bdc7ad-1599-4ccd-8abc-5536833571b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from Bio.Seq import Seq\n",
    "from transformers import T5Tokenizer, TFT5EncoderModel\n",
    "import re\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "import pickle\n",
    "import sys\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0749e1",
   "metadata": {
    "id": "5c0749e1"
   },
   "source": [
    "# Fine-Tuning Protein Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d81db83",
   "metadata": {
    "id": "1d81db83"
   },
   "source": [
    "In this notebook, we're going to do some transfer learning to fine-tune some large, pre-trained protein language models on tasks of interest. If that sentence feels a bit intimidating to you, don't panic - there's a blog post coming soon that explains the concepts here in much more detail.\n",
    "\n",
    "The specific model we're going to use is ESM-2, which is the state-of-the-art protein language model at the time of writing (November 2022). The citation for this model is [Lin et al, 2022](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).\n",
    "\n",
    "There are several ESM-2 checkpoints with differing model sizes. Larger models will generally have better accuracy, but they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints (at time of writing) are:\n",
    "\n",
    "| Checkpoint name | Num layers | Num parameters |\n",
    "|------------------------------|----|----------|\n",
    "| `esm2_t48_15B_UR50D`         | 48 | 15B     |\n",
    "| `esm2_t36_3B_UR50D`          | 36 | 3B      | \n",
    "| `esm2_t33_650M_UR50D`        | 33 | 650M    | \n",
    "| `esm2_t30_150M_UR50D`        | 30 | 150M    | \n",
    "| `esm2_t12_35M_UR50D`         | 12 | 35M     | \n",
    "| `esm2_t6_8M_UR50D`           | 6  | 8M      | \n",
    "\n",
    "Note that the larger checkpoints may be very difficult to train without a large cloud GPU like an A100 or H100, and the largest 15B parameter checkpoint will probably be impossible to train on **any** single GPU! Also, note that memory usage for attention during training will scale as `O(batch_size * num_layers * seq_len^2)`, so larger models on long sequences will use quite a lot of memory! We will use the `esm2_t12_35M_UR50D` checkpoint for this notebook, which should train on any Colab instance or modern GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32e605a2",
   "metadata": {
    "id": "32e605a2"
   },
   "outputs": [],
   "source": [
    "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\"\n",
    "# model_checkpoint = \"facebook/esm2_t48_15B_UR50D\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e6ac19",
   "metadata": {
    "id": "a8e6ac19"
   },
   "source": [
    "# Sequence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eb400c",
   "metadata": {
    "id": "c3eb400c"
   },
   "source": [
    "One of the most common tasks you can perform with a language model is **sequence classification**. In sequence classification, we classify an entire protein into a category, from a list of two or more possibilities. There's no limit on the number of categories you can use, or the specific problem you choose, as long as it's something the model could in theory infer from the raw protein sequence. To keep things simple for this example, though, let's try classifying proteins by their cellular localization - given their sequence, can we predict if they're going to be found in the cytosol (the fluid inside the cell) or embedded in the cell membrane?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bc122f",
   "metadata": {
    "id": "c5bc122f"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c91d394",
   "metadata": {
    "id": "4c91d394"
   },
   "source": [
    "In this section, we're going to gather some training data from UniProt. Our goal is to create a pair of lists: `sequences` and `labels`. `sequences` will be a list of protein sequences, which will just be strings like \"MNKL...\", where each letter represents a single amino acid in the complete protein. `labels` will be a list of the category for each sequence. The categories will just be integers, with 0 representing the first category, 1 representing the second and so on. In other words, if `sequences[i]` is a protein sequence then `labels[i]` should be its corresponding category. These will form the **training data** we're going to use to teach the model the task we want it to do.\n",
    "\n",
    "If you're adapting this notebook for your own use, this will probably be the main section you want to change! You can do whatever you want here, as long as you create those two lists by the end of it. If you want to follow along with this example, though, first we'll need to `import requests` and set up our query to UniProt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "550d85f1-6320-432d-8b20-b6531a0fc845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VHorVHH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QITLKESGPTLVKPTQTLTLTCKLSGFSVNTGGVGVGWIRQPPGKA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGDTFNIYAINWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFNSYAITWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>QVQLVESGGGVVQPGRSLRLSCAASGFTFSTHGMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFRRYAISWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11862</th>\n",
       "      <td>EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11863</th>\n",
       "      <td>QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11864</th>\n",
       "      <td>QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11865</th>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11866</th>\n",
       "      <td>QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11415 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 VHorVHH\n",
       "5      QITLKESGPTLVKPTQTLTLTCKLSGFSVNTGGVGVGWIRQPPGKA...\n",
       "32     QVQLVQSGAEVKKPGSSVKVSCKASGDTFNIYAINWVRQAPGQGLE...\n",
       "33     QVQLVQSGAEVKKPGSSVKVSCKASGGTFNSYAITWVRQAPGQGLE...\n",
       "34     QVQLVESGGGVVQPGRSLRLSCAASGFTFSTHGMHWVRQAPGKGLE...\n",
       "35     QVQLVQSGAEVKKPGSSVKVSCKASGGTFRRYAISWVRQAPGQGLE...\n",
       "...                                                  ...\n",
       "11862  EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...\n",
       "11863  QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...\n",
       "11864  QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...\n",
       "11865  EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...\n",
       "11866  QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...\n",
       "\n",
       "[11415 rows x 1 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/CoV-AbDab_031022.csv\")\n",
    "df = df[[\"VHorVHH\"]]\n",
    "df = df[df[\"VHorVHH\"].apply(lambda x: len(x) <= 138)]\n",
    "df = df[(df.VHorVHH != 'ND')]\n",
    "df\n",
    "# df = df[[\"CDRH3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca827dc2-3d2e-4bbd-80ae-08b1ea26e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/Bio/Seq.py:3482: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVQLVQSGPEVKKPGSSVKVSCKASGGTFSNFAFSWVRQAPGQGLEWMGSVILHLGTSTYAQKFQGRVTITADESTSAAFMDLNALTSDDTAVYYCARVVAVPGRVPYWFDPWGQGTLVTVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARVPPTSTVTTLGDDYWGQGTLVTVSS', 'QVQLVQSGPEVKKPGASVRVSCKPSGYPFSNYGISWMRQAPGQGLEWMGWVNIDKGNTKYAQKFQDRVTMTTDTSSSTVYLELRSLRSDDTALYYCARERGGYRYGDYWGQGTLVIVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEIKHSGSTNYIPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCASRAGAAAASWGQGTLVTVSS', 'SETLSLTCAVHGGSFSDYYWTWIRQPPGKGLEWIGEINHRGGTNYNPSLKSRLNILVDTSKSQFSLKLSSVTAADTAVYFCARERFILIRGLTKYYYYMDVWGKGTTVTVS'] 11415\n"
     ]
    }
   ],
   "source": [
    "dummy = []\n",
    "head = []\n",
    "with open(\"../Data/cAb-rep/cAb-Rep_heavy.nt.txt\") as myfile:\n",
    "    # count = 0\n",
    "    for i in myfile:\n",
    "        # if count <= 1:\n",
    "        #     print(i)\n",
    "        #     if i.find(\">\") == -1 & i.find(\"-\") == -1:\n",
    "        #         print(Seq.translate(i.strip()))\n",
    "        #     count+=1\n",
    "        dummy.append(i)\n",
    "    np.random.shuffle(dummy)\n",
    "    \n",
    "    for i in dummy:\n",
    "        if i.find(\">\") == -1 & i.find(\"-\") == -1 & i.find(\"N\") == -1: # These conditions must be met for a valid sequence, the longest was 141. However, there is no 141 sequence for COVID, the greatest is 138, so we go with that\n",
    "            aa_sequence = Seq.translate(i.strip())\n",
    "            if (len(aa_sequence) <= 138) & (len(aa_sequence) >= 100):\n",
    "                head.append(aa_sequence)\n",
    "                if len(head) >= 11415:\n",
    "                    break\n",
    "print(head[:5], len(head))\n",
    "healthy_sequences = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee3c0779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del head\n",
    "del myfile\n",
    "del dummy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67483afe-f027-4dd9-9d03-e62e4145b446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "covid_sequences = df.to_numpy()\n",
    "covid_sequences = np.squeeze(covid_sequences)\n",
    "np.random.shuffle(covid_sequences)\n",
    "print(len(max(healthy_sequences, key=len)))\n",
    "print(len(max(covid_sequences, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a42d88a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b745d707-e2ff-4562-9e2d-a602b198b1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "healthy_lables = [0] * 11415\n",
    "covid_lables = [1] * 11415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e4d82be-b95e-4fa8-b69b-8595404f4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((healthy_sequences, covid_sequences))\n",
    "y = np.concatenate((healthy_lables, covid_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1335e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.tolist()\n",
    "y = y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "568d6d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del healthy_sequences\n",
    "del covid_sequences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aac39c",
   "metadata": {
    "id": "e0aac39c"
   },
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9099e7c",
   "metadata": {
    "id": "a9099e7c"
   },
   "source": [
    "Since the data we're loading isn't prepared for us as a machine learning dataset, we'll have to split the data into train and test sets ourselves! We can use sklearn's function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7452eec8-9c73-452a-b112-92a653d1fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22372ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d29b4ed",
   "metadata": {
    "id": "7d29b4ed"
   },
   "source": [
    "## Tokenizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02baaf7",
   "metadata": {
    "id": "c02baaf7"
   },
   "source": [
    "All inputs to neural nets must be numerical. The process of converting strings into numerical indices suitable for a neural net is called **tokenization**. For natural language this can be quite complex, as usually the network's vocabulary will not contain every possible word, which means the tokenizer must handle splitting rarer words into pieces, as well as all the complexities of capitalization and unicode characters and so on.\n",
    "\n",
    "With proteins, however, things are very easy. In protein language models, each amino acid is converted to a single token. Every model on `transformers` comes with an associated `tokenizer` that handles tokenization for it, and protein language models are no different. Let's get our tokenizer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ddbe2b2d",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "b66e9b0e44f34a59bc6e29961cd2b418",
      "fdfcf0f1f6ba4ef1b40691b7d51217cc",
      "0c669f05694948dc83b6a4efd112adc0"
     ]
    },
    "id": "ddbe2b2d",
    "outputId": "99c0510f-00cd-47c4-ad92-18bc024d822c"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16be37",
   "metadata": {
    "id": "9d16be37"
   },
   "source": [
    "Let's try a single sequence to see what the outputs from our tokenizer look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "687386af",
   "metadata": {
    "id": "687386af",
    "outputId": "aa4a3278-b638-4b87-c55c-0beaf8fa184b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 9, 7, 16, 4, 7, 9, 8, 6, 6, 6, 4, 12, 16, 14, 6, 6, 8, 4, 10, 4, 8, 23, 5, 5, 8, 6, 18, 11, 7, 8, 8, 17, 19, 20, 8, 22, 7, 10, 16, 5, 14, 6, 15, 6, 4, 9, 22, 7, 8, 7, 12, 19, 8, 6, 6, 8, 11, 19, 19, 5, 13, 8, 7, 15, 6, 10, 18, 11, 7, 8, 10, 13, 17, 8, 15, 17, 11, 4, 19, 4, 16, 20, 17, 8, 4, 10, 5, 9, 13, 11, 5, 7, 19, 19, 23, 5, 10, 6, 6, 10, 19, 13, 19, 13, 7, 18, 13, 12, 22, 6, 16, 6, 11, 20, 7, 11, 7, 8, 8, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a719808",
   "metadata": {
    "id": "9a719808"
   },
   "source": [
    "This looks good! We can see that our sequence has been converted into `input_ids`, which is the tokenized sequence, and an `attention_mask`. The attention mask handles the case when we have sequences of variable length - in those cases, the shorter sequences are padded with blank \"padding\" tokens, and the attention mask is padded with 0s to indicate that those tokens should be ignored by the model.\n",
    "\n",
    "So now, let's tokenize our whole dataset. Note that we don't need to do anything with the labels, as they're already in the format we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56e26ddf",
   "metadata": {
    "id": "56e26ddf"
   },
   "outputs": [],
   "source": [
    "train_tokenized = tokenizer(X_train)\n",
    "test_tokenized = tokenizer(X_test)\n",
    "val_tokenized = tokenizer(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3681d1",
   "metadata": {
    "id": "df3681d1"
   },
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85089e49",
   "metadata": {
    "id": "85089e49"
   },
   "source": [
    "Now we want to turn this data into a dataset that Keras can load samples from. We can use the HuggingFace `Dataset` class for this, which has convenience functions to wrap itself with a `tf.data.Dataset`, although there are a number of different approaches that you can take at this stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb79ba6c",
   "metadata": {
    "id": "fb79ba6c",
    "outputId": "83cea580-6d6b-4efe-d819-6c44da023763"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask'],\n",
       "    num_rows: 18492\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "val_dataset = Dataset.from_dict(val_tokenized)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e809e47",
   "metadata": {
    "id": "9e809e47"
   },
   "source": [
    "This looks good, but we're missing our labels! Let's add those on as an extra column to the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "090acc0d",
   "metadata": {
    "id": "090acc0d",
    "outputId": "bf732b0d-e9c1-44b6-9e14-37f099c29ea9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 18492\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.add_column(\"labels\", y_train) # train_labels = y_train\n",
    "test_dataset = test_dataset.add_column(\"labels\", y_test)\n",
    "val_dataset = val_dataset.add_column(\"labels\", y_val)\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced9aaa8",
   "metadata": {
    "id": "ced9aaa8"
   },
   "source": [
    "Looks good! We're ready for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af074a5c",
   "metadata": {
    "id": "af074a5c"
   },
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccab5d70",
   "metadata": {
    "id": "ccab5d70"
   },
   "source": [
    "Next, we want to load our model. Make sure to use exactly the same model as you used when loading the tokenizer, or your model might not understand the tokenization scheme you're using!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fc164b49",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "365446cb74e243a8b5555c688f6b2c20"
     ]
    },
    "id": "fc164b49",
    "outputId": "f1f0781d-04f9-4fab-9864-b67d1a666707"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num labels: 2\n",
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n",
      "maxCacheSize: 24.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 21:39:50.670485: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-01 21:39:50.670611: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "Some layers from the model checkpoint at facebook/esm2_t12_35M_UR50D were not used when initializing TFEsmForSequenceClassification: ['lm_head', 'esm/contact_head/regression/kernel:0', 'esm/contact_head/regression/bias:0']\n",
      "- This IS expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFEsmForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFEsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "num_labels = max(y_train + y_test) + 1  # Add 1 since 0 can be a label\n",
    "print(\"Num labels:\", num_labels)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dcba23",
   "metadata": {
    "id": "49dcba23"
   },
   "source": [
    "These warnings are telling us that the model is discarding some weights that it used for language modelling (the `lm_head`) and adding some weights for sequence classification (the `classifier`). This is exactly what we expect when we want to fine-tune a language model on a sequence classification task!\n",
    "\n",
    "Next, let's prepare our `tf.data.Dataset`. This Dataset will stream samples from our Huggingface `Dataset` in a way that Keras natively understands - once we've created it, we can pass it straight to `model.fit()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14fcf32d",
   "metadata": {
    "id": "14fcf32d"
   },
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_val_set = model.prepare_tf_dataset(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95d099",
   "metadata": {
    "id": "bc95d099"
   },
   "source": [
    "You might wonder why we pass along the `tokenizer` when we already preprocessed our data. This is because we will use it one last time to make all the samples we gather the same length by applying padding, which requires knowing the model's preferences regarding padding (to the left or right? with which token?). The `tokenizer` has a `pad()` method that will do all of this right for us, and `prepare_tf_dataset` will use it.\n",
    "\n",
    "Now all we need to do is compile our model. We use the `AdamWeightDecay` optimizer, which usually performs a little better than the base `Adam` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d36462cb",
   "metadata": {
    "id": "d36462cb",
    "outputId": "69d20080-4b34-453b-90ea-f6effab767ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_esm_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " esm (TFEsmMainLayer)        multiple                  33269424  \n",
      "                                                                 \n",
      " classifier (TFEsmClassifica  multiple                 231842    \n",
      " tionHead)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33,501,266\n",
      "Trainable params: 33,501,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "\n",
    "model.compile(optimizer=AdamWeightDecay(2e-5), metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709dcf25",
   "metadata": {
    "id": "709dcf25"
   },
   "source": [
    "And now we can fit our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e212b751",
   "metadata": {
    "id": "e212b751",
    "outputId": "29146681-2362-4a54-a6f2-cfde6f87c406",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 21:39:56.555602: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-01 21:40:08.089042: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2311/2311 [==============================] - ETA: 0s - loss: 0.4128 - accuracy: 0.7908"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-01 22:45:51.786571: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2311/2311 [==============================] - 4523s 2s/step - loss: 0.4128 - accuracy: 0.7908 - val_loss: 0.3468 - val_accuracy: 0.8229\n",
      "Epoch 2/3\n",
      "2311/2311 [==============================] - 4363s 2s/step - loss: 0.3356 - accuracy: 0.8397 - val_loss: 0.3220 - val_accuracy: 0.8443\n",
      "Epoch 3/3\n",
      "2311/2311 [==============================] - 4391s 2s/step - loss: 0.2879 - accuracy: 0.8676 - val_loss: 0.3039 - val_accuracy: 0.8574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2dd1a7bb0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_set, validation_data=tf_val_set, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfec59f4",
   "metadata": {
    "id": "dfec59f4"
   },
   "source": [
    "Nice! After three epochs we have a model accuracy of ~94%. Note that we didn't do a lot of work to filter the training data or tune hyperparameters for this experiment, and also that we used one of the smallest ESM-2 models. With a larger starting model and more effort to ensure that the training data categories were cleanly separable, accuracy could almost certainly go a lot higher!\n",
    "\n",
    "Now that we're done, let's see how we can upload our model to the HuggingFace Hub. This step is optional, but will allow us to easily share it with other researchers. If you encounter any errors here, make sure you ran the login cell at the top of the notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2214f8",
   "metadata": {
    "id": "2e2214f8"
   },
   "source": [
    "First, let's set a couple of properties on our model. This is optional, but will ensure the model knows the names of its categories, rather than just outputting \"0\" or \"1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ca836e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.label2id = {\"healthy\": 0, \"covid\": 1}\n",
    "model.id2label = {val: key for key, val in model.label2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868bf45",
   "metadata": {
    "id": "8868bf45"
   },
   "source": [
    "Now we can push it to the hub as simply as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "770c5bb1",
   "metadata": {
    "id": "770c5bb1",
    "outputId": "5eebae66-43c8-4d6e-c51f-0fd64ab8fe5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/josephyu12/esm2_t12_35M_UR50D-finetuned-healthy-covid-classification/commit/ed283aa168184a2f6cc288400ac64f0b9285a340', commit_message='Upload tokenizer', commit_description='', oid='ed283aa168184a2f6cc288400ac64f0b9285a340', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = model_checkpoint.split('/')[-1]\n",
    "finetuned_model_name = f\"{model_name}-finetuned-healthy-covid-classification\"\n",
    "\n",
    "model.push_to_hub(finetuned_model_name)\n",
    "tokenizer.push_to_hub(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fc1993",
   "metadata": {
    "id": "00fc1993"
   },
   "source": [
    "If you used the code above, you can now share this model with all your friends, family or favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"your-username/my-awesome-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1842d9ce",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2ef458",
   "metadata": {
    "id": "bc2ef458",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "***\n",
    "# Token classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d701ed",
   "metadata": {
    "id": "78d701ed"
   },
   "source": [
    "Another common language model task is **token classification**. In this task, instead of classifying the whole sequence into a single category, we categorize each token (amino acid, in this case!) into one or more categories. This kind of model could be useful for:\n",
    "\n",
    "- Predicting secondary structure\n",
    "- Predicting buried vs. exposed residues\n",
    "- Predicting residues that will receive post-translational modifications\n",
    "- Predicting residues involved in binding pockets or active sites\n",
    "- Probably several other things, it's been a while since I was a postdoc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e00afe",
   "metadata": {
    "id": "20e00afe"
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9e75c",
   "metadata": {
    "id": "f1b9e75c"
   },
   "source": [
    "In this section, we're going to gather some training data from UniProt. As in the sequence classification example, we aim to create two lists: `sequences` and `labels`. Unlike in that example, however, the `labels` are more than just single integers. Instead, the label for each sample will be **one integer per token in the input**. This should make sense - when we do token classification, different tokens in the input may have different categories!\n",
    "\n",
    "To demonstrate token classification, we're going to go back to UniProt and get some data on protein secondary structures. As above, this will probably the main section you want to change when adapting this code to your own problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf52cfb8",
   "metadata": {
    "id": "bf52cfb8"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "query_url =\"https://rest.uniprot.org/uniprotkb/stream?compressed=true&fields=accession%2Csequence%2Cft_strand%2Cft_helix&format=tsv&query=%28%28organism_id%3A9606%29%20AND%20%28reviewed%3Atrue%29%20AND%20%28length%3A%5B80%20TO%20500%5D%29%29\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c902be",
   "metadata": {
    "id": "73c902be"
   },
   "source": [
    "This time, our UniProt search was `(organism_id:9606) AND (reviewed:true) AND (length:[100 TO 1000])` as it was in the first example, but instead of `Subcellular location [CC]` we take the `Helix` and `Beta strand` columns, as they contain the secondary structure information we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be65f529",
   "metadata": {
    "id": "be65f529"
   },
   "outputs": [],
   "source": [
    "uniprot_request = requests.get(query_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f683dd7",
   "metadata": {
    "id": "3f683dd7"
   },
   "source": [
    "To get this data into Pandas, we use a `BytesIO` object, which Pandas will treat like a file. If you downloaded the data as a file you can skip this bit and just pass the filepath directly to `read_csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49439ab",
   "metadata": {
    "id": "f49439ab",
    "outputId": "1b4fb704-1861-4e13-c06d-e9a185754f45"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Beta strand</th>\n",
       "      <th>Helix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0K2S4Q6</td>\n",
       "      <td>MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A5B9</td>\n",
       "      <td>DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...</td>\n",
       "      <td>STRAND 9..14; /evidence=\"ECO:0007829|PDB:4UDT\"...</td>\n",
       "      <td>HELIX 2..4; /evidence=\"ECO:0007829|PDB:4UDT\"; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0AVI4</td>\n",
       "      <td>MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0JLT2</td>\n",
       "      <td>MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...</td>\n",
       "      <td>STRAND 79..81; /evidence=\"ECO:0007829|PDB:7EMF\"</td>\n",
       "      <td>HELIX 83..86; /evidence=\"ECO:0007829|PDB:7EMF\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0M8Q6</td>\n",
       "      <td>GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11977</th>\n",
       "      <td>Q9NZ38</td>\n",
       "      <td>MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11978</th>\n",
       "      <td>Q9UFV3</td>\n",
       "      <td>MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11979</th>\n",
       "      <td>Q9Y6C7</td>\n",
       "      <td>MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11980</th>\n",
       "      <td>X6R8D5</td>\n",
       "      <td>MGRKEHESPSQPHMCGWEDSQKPSVPSHGPKTPSCKGVKAPHSSRP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11981</th>\n",
       "      <td>X6R8R1</td>\n",
       "      <td>MGVVLSPHPAPSRREPLAPLAPGTRPGWSPAVSGSSRSALRPSTAG...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11982 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Entry                                           Sequence  \\\n",
       "0      A0A0K2S4Q6  MTQRAGAAMLPSALLLLCVPGCLTVSGPSTVMGAVGESLSVQCRYE...   \n",
       "1          A0A5B9  DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...   \n",
       "2          A0AVI4  MDSPEVTFTLAYLVFAVCFVFTPNEFHAAGLTVQNLLSGWLGSEDA...   \n",
       "3          A0JLT2  MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...   \n",
       "4          A0M8Q6  GQPKAAPSVTLFPPSSEELQANKATLVCLVSDFNPGAVTVAWKADG...   \n",
       "...           ...                                                ...   \n",
       "11977      Q9NZ38  MAFPGQSDTKMQWPEVPALPLLSSLCMAMVRKSSALGKEVGRRSEG...   \n",
       "11978      Q9UFV3  MAETYRRSRQHEQLPGQRHMDLLTGYSKLIQSRLKLLLHLGSQPPV...   \n",
       "11979      Q9Y6C7  MAHHSLNTFYIWHNNVLHTHLVFFLPHLLNQPFSRGSFLIWLLLCW...   \n",
       "11980      X6R8D5  MGRKEHESPSQPHMCGWEDSQKPSVPSHGPKTPSCKGVKAPHSSRP...   \n",
       "11981      X6R8R1  MGVVLSPHPAPSRREPLAPLAPGTRPGWSPAVSGSSRSALRPSTAG...   \n",
       "\n",
       "                                             Beta strand  \\\n",
       "0                                                    NaN   \n",
       "1      STRAND 9..14; /evidence=\"ECO:0007829|PDB:4UDT\"...   \n",
       "2                                                    NaN   \n",
       "3        STRAND 79..81; /evidence=\"ECO:0007829|PDB:7EMF\"   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "11977                                                NaN   \n",
       "11978                                                NaN   \n",
       "11979                                                NaN   \n",
       "11980                                                NaN   \n",
       "11981                                                NaN   \n",
       "\n",
       "                                                   Helix  \n",
       "0                                                    NaN  \n",
       "1      HELIX 2..4; /evidence=\"ECO:0007829|PDB:4UDT\"; ...  \n",
       "2                                                    NaN  \n",
       "3      HELIX 83..86; /evidence=\"ECO:0007829|PDB:7EMF\"...  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "11977                                                NaN  \n",
       "11978                                                NaN  \n",
       "11979                                                NaN  \n",
       "11980                                                NaN  \n",
       "11981                                                NaN  \n",
       "\n",
       "[11982 rows x 4 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import pandas\n",
    "\n",
    "bio = BytesIO(uniprot_request.content)\n",
    "\n",
    "df = pandas.read_csv(bio, compression='gzip', sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736010f0",
   "metadata": {
    "id": "736010f0"
   },
   "source": [
    "Since not all proteins have this structural information, we discard proteins that have no annotated beta strands or alpha helices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce9a5c",
   "metadata": {
    "id": "39ce9a5c",
    "outputId": "db8f16aa-61b3-4ef4-bd8c-60753eabad36"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entry</th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Beta strand</th>\n",
       "      <th>Helix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A5B9</td>\n",
       "      <td>DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...</td>\n",
       "      <td>STRAND 9..14; /evidence=\"ECO:0007829|PDB:4UDT\"...</td>\n",
       "      <td>HELIX 2..4; /evidence=\"ECO:0007829|PDB:4UDT\"; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0JLT2</td>\n",
       "      <td>MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...</td>\n",
       "      <td>STRAND 79..81; /evidence=\"ECO:0007829|PDB:7EMF\"</td>\n",
       "      <td>HELIX 83..86; /evidence=\"ECO:0007829|PDB:7EMF\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>A1L3X0</td>\n",
       "      <td>MAFSDLTSRTVHLYDNWIKDADPRVEDWLLMSSPLPQTILLGFYVY...</td>\n",
       "      <td>STRAND 97..99; /evidence=\"ECO:0007829|PDB:6Y7F\"</td>\n",
       "      <td>HELIX 17..20; /evidence=\"ECO:0007829|PDB:6Y7F\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>A1Z1Q3</td>\n",
       "      <td>MYPSNKKKKVWREEKERLLKMTLEERRKEYLRDYIPLNSILSWKEE...</td>\n",
       "      <td>STRAND 71..77; /evidence=\"ECO:0007829|PDB:4IQY...</td>\n",
       "      <td>HELIX 11..19; /evidence=\"ECO:0007829|PDB:4IQY\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>A2RUC4</td>\n",
       "      <td>MAGQHLPVPRLEGVSREQFMQHLYPQRKPLVLEGIDLGPCTSKWTV...</td>\n",
       "      <td>STRAND 10..13; /evidence=\"ECO:0007829|PDB:3AL5...</td>\n",
       "      <td>HELIX 16..22; /evidence=\"ECO:0007829|PDB:3AL5\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11551</th>\n",
       "      <td>Q96I45</td>\n",
       "      <td>MVNLGLSRVDDAVAAKHPGLGEYAACQSHAFMKGVFTFVTGTGMAF...</td>\n",
       "      <td>STRAND 3..5; /evidence=\"ECO:0007829|PDB:2LOR\";...</td>\n",
       "      <td>HELIX 6..16; /evidence=\"ECO:0007829|PDB:2LOR\";...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11614</th>\n",
       "      <td>Q9H0W7</td>\n",
       "      <td>MPTNCAAAGCATTYNKHINISFHRFPLDPKRRKEWVRLVRRKNFVP...</td>\n",
       "      <td>STRAND 7..9; /evidence=\"ECO:0007829|PDB:2D8R\";...</td>\n",
       "      <td>HELIX 29..38; /evidence=\"ECO:0007829|PDB:2D8R\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11659</th>\n",
       "      <td>Q9P1F3</td>\n",
       "      <td>MNVDHEVNLLVEEIHRLGSKNADGKLSVKFGVLFRDDKCANLFEAL...</td>\n",
       "      <td>STRAND 24..29; /evidence=\"ECO:0007829|PDB:2L2O...</td>\n",
       "      <td>HELIX 3..17; /evidence=\"ECO:0007829|PDB:2L2O\";...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11661</th>\n",
       "      <td>Q9P298</td>\n",
       "      <td>MSANRRWWVPPDDEDCVSEKLLRKTRESPLVPIGLGGCLVVAAYRI...</td>\n",
       "      <td>STRAND 11..14; /evidence=\"ECO:0007829|PDB:2LON...</td>\n",
       "      <td>HELIX 18..24; /evidence=\"ECO:0007829|PDB:2LON\"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11668</th>\n",
       "      <td>Q9UIY3</td>\n",
       "      <td>MSASVKESLQLQLLEMEMLFSMFPNQGEVKLEDVNALTNIKRYLEG...</td>\n",
       "      <td>STRAND 28..32; /evidence=\"ECO:0007829|PDB:2DAW...</td>\n",
       "      <td>HELIX 5..22; /evidence=\"ECO:0007829|PDB:2DAW\";...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3911 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entry                                           Sequence  \\\n",
       "1      A0A5B9  DLKNVFPPKVAVFEPSEAEISHTQKATLVCLATGFYPDHVELSWWV...   \n",
       "3      A0JLT2  MENFTALFGAQADPPPPPTALGFGPGKPPPPPPPPAGGGPGTAPPP...   \n",
       "14     A1L3X0  MAFSDLTSRTVHLYDNWIKDADPRVEDWLLMSSPLPQTILLGFYVY...   \n",
       "16     A1Z1Q3  MYPSNKKKKVWREEKERLLKMTLEERRKEYLRDYIPLNSILSWKEE...   \n",
       "20     A2RUC4  MAGQHLPVPRLEGVSREQFMQHLYPQRKPLVLEGIDLGPCTSKWTV...   \n",
       "...       ...                                                ...   \n",
       "11551  Q96I45  MVNLGLSRVDDAVAAKHPGLGEYAACQSHAFMKGVFTFVTGTGMAF...   \n",
       "11614  Q9H0W7  MPTNCAAAGCATTYNKHINISFHRFPLDPKRRKEWVRLVRRKNFVP...   \n",
       "11659  Q9P1F3  MNVDHEVNLLVEEIHRLGSKNADGKLSVKFGVLFRDDKCANLFEAL...   \n",
       "11661  Q9P298  MSANRRWWVPPDDEDCVSEKLLRKTRESPLVPIGLGGCLVVAAYRI...   \n",
       "11668  Q9UIY3  MSASVKESLQLQLLEMEMLFSMFPNQGEVKLEDVNALTNIKRYLEG...   \n",
       "\n",
       "                                             Beta strand  \\\n",
       "1      STRAND 9..14; /evidence=\"ECO:0007829|PDB:4UDT\"...   \n",
       "3        STRAND 79..81; /evidence=\"ECO:0007829|PDB:7EMF\"   \n",
       "14       STRAND 97..99; /evidence=\"ECO:0007829|PDB:6Y7F\"   \n",
       "16     STRAND 71..77; /evidence=\"ECO:0007829|PDB:4IQY...   \n",
       "20     STRAND 10..13; /evidence=\"ECO:0007829|PDB:3AL5...   \n",
       "...                                                  ...   \n",
       "11551  STRAND 3..5; /evidence=\"ECO:0007829|PDB:2LOR\";...   \n",
       "11614  STRAND 7..9; /evidence=\"ECO:0007829|PDB:2D8R\";...   \n",
       "11659  STRAND 24..29; /evidence=\"ECO:0007829|PDB:2L2O...   \n",
       "11661  STRAND 11..14; /evidence=\"ECO:0007829|PDB:2LON...   \n",
       "11668  STRAND 28..32; /evidence=\"ECO:0007829|PDB:2DAW...   \n",
       "\n",
       "                                                   Helix  \n",
       "1      HELIX 2..4; /evidence=\"ECO:0007829|PDB:4UDT\"; ...  \n",
       "3      HELIX 83..86; /evidence=\"ECO:0007829|PDB:7EMF\"...  \n",
       "14     HELIX 17..20; /evidence=\"ECO:0007829|PDB:6Y7F\"...  \n",
       "16     HELIX 11..19; /evidence=\"ECO:0007829|PDB:4IQY\"...  \n",
       "20     HELIX 16..22; /evidence=\"ECO:0007829|PDB:3AL5\"...  \n",
       "...                                                  ...  \n",
       "11551  HELIX 6..16; /evidence=\"ECO:0007829|PDB:2LOR\";...  \n",
       "11614     HELIX 29..38; /evidence=\"ECO:0007829|PDB:2D8R\"  \n",
       "11659  HELIX 3..17; /evidence=\"ECO:0007829|PDB:2L2O\";...  \n",
       "11661  HELIX 18..24; /evidence=\"ECO:0007829|PDB:2LON\"...  \n",
       "11668  HELIX 5..22; /evidence=\"ECO:0007829|PDB:2DAW\";...  \n",
       "\n",
       "[3911 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_structure_rows = df[\"Beta strand\"].isna() & df[\"Helix\"].isna()\n",
    "df = df[~no_structure_rows]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e372c",
   "metadata": {
    "id": "f43e372c"
   },
   "source": [
    "Well, this works, but that data still isn't in a clean format that we can use to build our labels. Let's take a look at one sample to see what exactly we're dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e99d1b",
   "metadata": {
    "id": "73e99d1b",
    "outputId": "9b7b049e-bf52-4dc7-fde4-216ad58d2319"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HELIX 2..4; /evidence=\"ECO:0007829|PDB:4UDT\"; HELIX 17..23; /evidence=\"ECO:0007829|PDB:4UDT\"; HELIX 83..86; /evidence=\"ECO:0007829|PDB:4UDT\"'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"Helix\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd5160a",
   "metadata": {
    "id": "6cd5160a"
   },
   "source": [
    "We'll need to use a [regex](https://docs.python.org/3/howto/regex.html) to pull out each segment that's marked as being a STRAND or HELIX. What we're asking for is a list of everywhere we see the word STRAND or HELIX followed by two numbers separated by two dots. In each case where this pattern is found, we tell the regex to extract the two numbers as a tuple for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7540949e",
   "metadata": {
    "id": "7540949e",
    "outputId": "c0ffd0f0-aa19-462d-f5cd-33ffb1e334c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', '4'), ('17', '23'), ('83', '86')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "strand_re = r\"STRAND\\s(\\d+)\\.\\.(\\d+)\\;\"\n",
    "helix_re = r\"HELIX\\s(\\d+)\\.\\.(\\d+)\\;\"\n",
    "\n",
    "re.findall(helix_re, df.iloc[0][\"Helix\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4457b1a0",
   "metadata": {
    "id": "4457b1a0"
   },
   "source": [
    "Looks good! We can use this to build our training data. Recall that the **labels** need to be a list or array of integers that's the same length as the input sequence. We're going to use 0 to indicate residues without any annotated structure, 1 for residues in an alpha helix, and 2 for residues in a beta strand. To build that, we'll start with an array of all 0s, and then fill in values based on the positions that our regex pulls out of the UniProt results.\n",
    "\n",
    "We'll use NumPy arrays rather than lists here, since these allow [slice assignment](https://numpy.org/doc/stable/user/basics.indexing.html#assigning-values-to-indexed-arrays), which will be a lot simpler than editing a list of integers. Note also that UniProt annotates residues starting from 1 (unlike Python, which starts from 0), and region annotations are inclusive (so 1..3 means residues 1, 2 and 3). To turn these into Python slices, we subtract 1 from the start of each annotation, but not the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c97179",
   "metadata": {
    "id": "a4c97179"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def build_labels(sequence, strands, helices):\n",
    "    # Start with all 0s\n",
    "    labels = np.zeros(len(sequence), dtype=np.int64)\n",
    "    \n",
    "    if isinstance(helices, float): # Indicates missing (NaN)\n",
    "        found_helices = []\n",
    "    else:\n",
    "        found_helices = re.findall(helix_re, helices)\n",
    "    for helix_start, helix_end in found_helices:\n",
    "        helix_start = int(helix_start) - 1\n",
    "        helix_end = int(helix_end)\n",
    "        assert helix_end <= len(sequence)\n",
    "        labels[helix_start: helix_end] = 1  # Helix category\n",
    "    \n",
    "    if isinstance(strands, float): # Indicates missing (NaN)\n",
    "        found_strands = []\n",
    "    else:\n",
    "        found_strands = re.findall(strand_re, strands)\n",
    "    for strand_start, strand_end in found_strands:\n",
    "        strand_start = int(strand_start) - 1\n",
    "        strand_end = int(strand_end)\n",
    "        assert strand_end <= len(sequence)\n",
    "        labels[strand_start: strand_end] = 2  # Strand category\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad7e7fd",
   "metadata": {
    "id": "5ad7e7fd"
   },
   "source": [
    "Now we've defined a helper function, let's build our lists of sequences and labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313811fe",
   "metadata": {
    "id": "313811fe"
   },
   "outputs": [],
   "source": [
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "for row_idx, row in df.iterrows():\n",
    "    row_labels = build_labels(row[\"Sequence\"], row[\"Beta strand\"], row[\"Helix\"])\n",
    "    sequences.append(row[\"Sequence\"])\n",
    "    labels.append(row_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b3ba8",
   "metadata": {
    "id": "8e8b3ba8"
   },
   "source": [
    "## Creating our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e619d9ae",
   "metadata": {
    "id": "e619d9ae"
   },
   "source": [
    "Nice! Now we'll split and tokenize the data, and then create datasets - I'll go through this quite quickly here, since it's identical to how we did it in the sequence classification example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c208c30",
   "metadata": {
    "id": "3c208c30"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2182fae2",
   "metadata": {
    "id": "2182fae2"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "train_tokenized = tokenizer(train_sequences)\n",
    "test_tokenized = tokenizer(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939f13a",
   "metadata": {
    "id": "3939f13a"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "test_dataset = Dataset.from_dict(test_tokenized)\n",
    "\n",
    "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
    "test_dataset = test_dataset.add_column(\"labels\", test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766fe4b",
   "metadata": {
    "id": "4766fe4b"
   },
   "source": [
    "## Model loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8419b5",
   "metadata": {
    "id": "de8419b5"
   },
   "source": [
    "The key difference here with the above example is that we use `TFAutoModelForTokenClassification` instead of `TFAutoModelForSequenceClassification`. We will also need a `data_collator` this time, as we're in the slightly more complex case where both inputs and labels must be padded in each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b26b828",
   "metadata": {
    "id": "4b26b828",
    "outputId": "df3b2b74-4125-4a30-cd73-7972df5131e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at facebook/esm2_t12_35M_UR50D were not used when initializing TFEsmForTokenClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFEsmForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFEsmForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFEsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['dropout_73', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "num_labels = 3\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec0005a",
   "metadata": {
    "id": "eec0005a"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c7305",
   "metadata": {
    "id": "bd3c7305"
   },
   "source": [
    "Now we create our `tf.data.Dataset` objects as before. Remember to pass the data collator, though! Note that when you pass a data collator, there's no need to pass your tokenizer, as the data collator is handling padding for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7724323",
   "metadata": {
    "id": "e7724323",
    "outputId": "68a5784a-290d-448e-e39d-b2f401435f15"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matt/PycharmProjects/transformers/src/transformers/tokenization_utils_base.py:715: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    train_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5fba9a",
   "metadata": {
    "id": "fb5fba9a"
   },
   "source": [
    "Our metrics are bit more complex than in the sequence classification task, as we need to ignore padding tokens (those where the label is `-100`). This means we'll need our own metric function where we only compute accuracy on non-padding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736886a0",
   "metadata": {
    "id": "736886a0",
    "outputId": "765ede89-3312-41ce-fa01-82dbdb938eb1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamWeightDecay\n",
    "import tensorflow as tf\n",
    "\n",
    "def masked_accuracy(y_true, y_pred):\n",
    "    predictions = tf.math.argmax(y_pred, axis=-1)  # Highest logit corresponds to predicted category\n",
    "    numerator = tf.math.count_nonzero((predictions == tf.cast(y_true, predictions.dtype)) & (y_true != -100), dtype=tf.float32)\n",
    "    denominator = tf.math.count_nonzero(y_true != -100, dtype=tf.float32)\n",
    "    return numerator / denominator\n",
    "\n",
    "model.compile(optimizer=AdamWeightDecay(2e-5), metrics=[masked_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37491af5",
   "metadata": {
    "id": "37491af5"
   },
   "source": [
    "And now we're ready to train our model! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c97836c",
   "metadata": {
    "id": "4c97836c",
    "outputId": "dcefac02-7625-465b-8e50-f2f5b884a110"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "366/366 [==============================] - 78s 184ms/step - loss: 0.5809 - masked_accuracy: 0.7502 - val_loss: 0.4764 - val_masked_accuracy: 0.8023\n",
      "Epoch 2/3\n",
      "366/366 [==============================] - 65s 177ms/step - loss: 0.4534 - masked_accuracy: 0.8132 - val_loss: 0.4564 - val_masked_accuracy: 0.8115\n",
      "Epoch 3/3\n",
      "366/366 [==============================] - 64s 176ms/step - loss: 0.4108 - masked_accuracy: 0.8325 - val_loss: 0.4586 - val_masked_accuracy: 0.8119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f60a011e320>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf_train_set, validation_data=tf_test_set, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f503fc00",
   "metadata": {
    "id": "f503fc00"
   },
   "source": [
    "This definitely seems harder than the first task, but we still attain a very respectable accuracy. Remember that to keep this demo lightweight, we used one of the smallest ESM models, focused on human proteins only and didn't put a lot of work into making sure we only included completely-annotated proteins in our training set. With a bigger model and a cleaner, broader training set, accuracy on this task could definitely go a lot higher!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb2a34",
   "metadata": {
    "id": "b1eb2a34"
   },
   "source": [
    "Now, let's push this model to the hub as we did before, while also setting the category labels appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8047203",
   "metadata": {
    "id": "a8047203",
    "outputId": "c1531bfe-f168-4e3d-df7c-f4654d7e2d50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Rocketknight1/esm2_t12_35M_UR50D-finetuned-secondary-structure-classification/commit/42093032cc6f061e1ef23fdf96ad80e5dce1a75a', commit_message='Upload tokenizer', commit_description='', oid='42093032cc6f061e1ef23fdf96ad80e5dce1a75a', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.label2id = {\"unstructured\": 0, \"helix\": 1, \"strand\": 2}\n",
    "model.id2label = {val: key for key, val in model.label2id.items()}\n",
    "\n",
    "model_name = model_checkpoint.split('/')[-1]\n",
    "finetuned_model_name = f\"{model_name}-finetuned-secondary-structure-classification\"\n",
    "\n",
    "model.push_to_hub(finetuned_model_name)\n",
    "tokenizer.push_to_hub(finetuned_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f05187",
   "metadata": {
    "id": "40f05187"
   },
   "source": [
    "If you used the code above, you can now share this model with all your friends, family or favorite pets: they can all load it with the identifier `\"your-username/the-name-you-picked\"` so for instance:\n",
    "\n",
    "```python\n",
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\"your-username/my-awesome-model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea261b9",
   "metadata": {
    "id": "8ea261b9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c38c8779811d1cfaf4b5a784c97578f212c26cffc36ab1ef679f872ba1fdba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
