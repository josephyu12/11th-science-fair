{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The examples in this notebook use a set of nine benchmarks described in our publication.\n",
    "# These benchmarks can be downloaded via FTP from: ftp.cs.huji.ac.il/users/nadavb/protein_bert/protein_benchmarks\n",
    "# Download the benchmarks into a directory on your machine and set the following variable to the path of that directory.\n",
    "BENCHMARKS_DIR = '/Users/joseph/Desktop/Academics/11th_Grade/STEM/ML/Data/peptide_benchmarks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75bdc7ad-1599-4ccd-8abc-5536833571b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from Bio.Seq import Seq\n",
    "np.random.seed(42)\n",
    "from transformers import TFBertModel, BertTokenizer,BertConfig, AutoTokenizer, TFAutoModelForSequenceClassification, AdamWeightDecay\n",
    "import re\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "import pickle\n",
    "import sys\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune the model for the signal peptide benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                                seq\n",
      "5662       0  MKSAASSEDVVRVTEDLSECRNRLDAGIEENRRNRQVIQDINDQLQ...\n",
      "1111       0  MENSSAASASSEAGSSRSQEIEELERFIDSYVLEYQVQGLLADKTE...\n",
      "15681      1  MRKSLLAILAVSSLVFSSASFAADLEDNMETLNDNLKVIEKADNAA...\n",
      "11591      0  MSEEDNRLNELFANDLGVQPPCERSLKEGRRFLNDFEIAAKKKLLE...\n",
      "9901       1  MFKKRGRQTVLIAAVLAFFTASSPLLARTQGEPTQVQQKLAALEKQ...\n",
      "...      ...                                                ...\n",
      "2622       0  MKLKKFLVLLLIPFLYACSSDRSGNYDDAFAKDTNGLDLLTGQFSQ...\n",
      "3702       1  MRSAAVLALLLCAGQVIALPVNSPMNKGDTEVMKCIVEVISDTLSK...\n",
      "12384      0  MVQRLLPGAHICRRSFNSSAIIKSSALTLKEALENVIPKKRDAVKK...\n",
      "10115      0  MTEQLQMTRRTMLAGAALAGAVAPLLHTAQAHAAGAAAAAGAAPVD...\n",
      "4621       0  MCGILAVHHVAEDIEAFKPKALHLSKQLRHRGPDWSGKAIRNQTIL...\n",
      "\n",
      "[14945 rows x 2 columns]\n",
      "14945 training set records, 1661 validation set records, 4152 test set records.\n"
     ]
    }
   ],
   "source": [
    "BENCHMARK_NAME = 'signalP_binary'\n",
    "\n",
    "# A local (non-global) binary output\n",
    "OUTPUT_TYPE = OutputType(False, 'binary')\n",
    "UNIQUE_LABELS = [0, 1]\n",
    "OUTPUT_SPEC = OutputSpec(OUTPUT_TYPE, UNIQUE_LABELS)\n",
    "\n",
    "\n",
    "# Loading the dataset\n",
    "\n",
    "train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % BENCHMARK_NAME)\n",
    "train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "train_set, valid_set = train_test_split(train_set, stratify = train_set['label'], test_size = 0.1, random_state = 0)\n",
    "print(train_set)\n",
    "\n",
    "test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % BENCHMARK_NAME)\n",
    "test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "\n",
    "print(f'{len(train_set)} training set records, {len(valid_set)} validation set records, {len(test_set)} test set records.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550d85f1-6320-432d-8b20-b6531a0fc845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VHorVHH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>QITLKESGPTLVKPTQTLTLTCKLSGFSVNTGGVGVGWIRQPPGKA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGDTFNIYAINWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFNSYAITWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>QVQLVESGGGVVQPGRSLRLSCAASGFTFSTHGMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>QVQLVQSGAEVKKPGSSVKVSCKASGGTFRRYAISWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11862</th>\n",
       "      <td>EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11863</th>\n",
       "      <td>QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11864</th>\n",
       "      <td>QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11865</th>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11866</th>\n",
       "      <td>QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11415 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 VHorVHH\n",
       "5      QITLKESGPTLVKPTQTLTLTCKLSGFSVNTGGVGVGWIRQPPGKA...\n",
       "32     QVQLVQSGAEVKKPGSSVKVSCKASGDTFNIYAINWVRQAPGQGLE...\n",
       "33     QVQLVQSGAEVKKPGSSVKVSCKASGGTFNSYAITWVRQAPGQGLE...\n",
       "34     QVQLVESGGGVVQPGRSLRLSCAASGFTFSTHGMHWVRQAPGKGLE...\n",
       "35     QVQLVQSGAEVKKPGSSVKVSCKASGGTFRRYAISWVRQAPGQGLE...\n",
       "...                                                  ...\n",
       "11862  EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...\n",
       "11863  QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...\n",
       "11864  QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...\n",
       "11865  EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...\n",
       "11866  QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...\n",
       "\n",
       "[11415 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/CoV-AbDab_031022.csv\")\n",
    "df = df[[\"VHorVHH\"]]\n",
    "df = df[df[\"VHorVHH\"].apply(lambda x: len(x) <= 138)]\n",
    "df = df[(df.VHorVHH != 'ND')]\n",
    "df\n",
    "# df = df[[\"CDRH3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca827dc2-3d2e-4bbd-80ae-08b1ea26e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/Bio/Seq.py:3482: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVQLVQSGPEVKKPGSSVKVSCKASGGTFSNFAFSWVRQAPGQGLEWMGSVILHLGTSTYAQKFQGRVTITADESTSAAFMDLNALTSDDTAVYYCARVVAVPGRVPYWFDPWGQGTLVTVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARVPPTSTVTTLGDDYWGQGTLVTVSS', 'QVQLVQSGPEVKKPGASVRVSCKPSGYPFSNYGISWMRQAPGQGLEWMGWVNIDKGNTKYAQKFQDRVTMTTDTSSSTVYLELRSLRSDDTALYYCARERGGYRYGDYWGQGTLVIVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEIKHSGSTNYIPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCASRAGAAAASWGQGTLVTVSS', 'SETLSLTCAVHGGSFSDYYWTWIRQPPGKGLEWIGEINHRGGTNYNPSLKSRLNILVDTSKSQFSLKLSSVTAADTAVYFCARERFILIRGLTKYYYYMDVWGKGTTVTVS'] 11415\n"
     ]
    }
   ],
   "source": [
    "dummy = []\n",
    "head = []\n",
    "with open(\"../Data/cAb-rep/cAb-Rep_heavy.nt.txt\") as myfile:\n",
    "    # count = 0\n",
    "    for i in myfile:\n",
    "        # if count <= 1:\n",
    "        #     print(i)\n",
    "        #     if i.find(\">\") == -1 & i.find(\"-\") == -1:\n",
    "        #         print(Seq.translate(i.strip()))\n",
    "        #     count+=1\n",
    "        dummy.append(i)\n",
    "    np.random.shuffle(dummy)\n",
    "    \n",
    "    for i in dummy:\n",
    "        if i.find(\">\") == -1 & i.find(\"-\") == -1 & i.find(\"N\") == -1: # These conditions must be met for a valid sequence, the longest was 141. However, there is no 141 sequence for COVID, the greatest is 138, so we go with that\n",
    "            aa_sequence = Seq.translate(i.strip())\n",
    "            if (len(aa_sequence) <= 138) & (len(aa_sequence) >= 100):\n",
    "                head.append(aa_sequence)\n",
    "                if len(head) >= 11415:\n",
    "                    break\n",
    "print(head[:5], len(head))\n",
    "healthy_sequences = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee3c0779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del head\n",
    "del myfile\n",
    "del dummy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67483afe-f027-4dd9-9d03-e62e4145b446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "covid_sequences = df.to_numpy()\n",
    "covid_sequences = np.squeeze(covid_sequences)\n",
    "np.random.shuffle(covid_sequences)\n",
    "print(len(max(healthy_sequences, key=len)))\n",
    "print(len(max(covid_sequences, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a42d88a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b745d707-e2ff-4562-9e2d-a602b198b1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "healthy_lables = [0] * 11415\n",
    "covid_lables = [1] * 11415"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e4d82be-b95e-4fa8-b69b-8595404f4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((healthy_sequences, covid_sequences))\n",
    "y = np.concatenate((healthy_lables, covid_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1335e81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.tolist()\n",
    "y = y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "568d6d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del healthy_sequences\n",
    "del covid_sequences\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7452eec8-9c73-452a-b112-92a653d1fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22372ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023_01_02-21:45:26] Training set: Filtered out 0 of 18492 (0.0%) records of lengths exceeding 510.\n",
      "[2023_01_02-21:45:26] Validation set: Filtered out 0 of 2055 (0.0%) records of lengths exceeding 510.\n",
      "[2023_01_02-21:45:26] Training with frozen pretrained layers...\n",
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n",
      "maxCacheSize: 24.00 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 21:45:26.740193: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-02 21:45:26.740346: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 22:07:45.340190: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-02 22:07:49.177108: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - ETA: 0s - loss: 0.5789WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 22:12:32.874813: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 301s 501ms/step - loss: 0.5789 - val_loss: 0.4532 - lr: 0.0100\n",
      "Epoch 2/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.5176WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 56s 97ms/step - loss: 0.5176 - val_loss: 0.4211 - lr: 0.0100\n",
      "Epoch 3/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.5104WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 56s 96ms/step - loss: 0.5104 - val_loss: 0.4210 - lr: 0.0100\n",
      "Epoch 4/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.5005WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 55s 95ms/step - loss: 0.5005 - val_loss: 0.4117 - lr: 0.0100\n",
      "Epoch 5/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.5089WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 56s 96ms/step - loss: 0.5089 - val_loss: 0.4019 - lr: 0.0100\n",
      "Epoch 6/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.5096WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "578/578 [==============================] - 55s 96ms/step - loss: 0.5096 - val_loss: 0.4104 - lr: 0.0100\n",
      "Epoch 7/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.4218WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "578/578 [==============================] - 55s 95ms/step - loss: 0.4218 - val_loss: 0.5107 - lr: 0.0025\n",
      "Epoch 8/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.4058WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 55s 96ms/step - loss: 0.4058 - val_loss: 0.3974 - lr: 6.2500e-04\n",
      "Epoch 9/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.4049WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "578/578 [==============================] - 55s 96ms/step - loss: 0.4049 - val_loss: 0.4037 - lr: 6.2500e-04\n",
      "Epoch 10/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3992WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 55s 96ms/step - loss: 0.3992 - val_loss: 0.3957 - lr: 1.5625e-04\n",
      "Epoch 11/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3960WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "578/578 [==============================] - 56s 96ms/step - loss: 0.3960 - val_loss: 0.4053 - lr: 1.5625e-04\n",
      "Epoch 12/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3956WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "578/578 [==============================] - 55s 95ms/step - loss: 0.3956 - val_loss: 0.3992 - lr: 3.9062e-05\n",
      "Epoch 13/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3956WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 56s 96ms/step - loss: 0.3956 - val_loss: 0.3991 - lr: 1.0000e-05\n",
      "Epoch 14/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3975WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 56s 97ms/step - loss: 0.3975 - val_loss: 0.3996 - lr: 1.0000e-05\n",
      "Epoch 15/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3949WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 56s 97ms/step - loss: 0.3949 - val_loss: 0.4008 - lr: 1.0000e-05\n",
      "[2023_01_02-22:25:43] Training the entire fine-tuned model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 22:25:51.091074: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023_01_02-22:26:22] Incompatible number of optimizer weights - will not initialize them.\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 23:15:37.299670: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - ETA: 0s - loss: 0.4181WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-02 23:27:24.777232: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "578/578 [==============================] - 733s 1s/step - loss: 0.4181 - val_loss: 0.3771 - lr: 1.0000e-04\n",
      "Epoch 2/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3699WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 181s 311ms/step - loss: 0.3699 - val_loss: 0.3537 - lr: 1.0000e-04\n",
      "Epoch 3/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3523WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 180s 311ms/step - loss: 0.3523 - val_loss: 0.3447 - lr: 1.0000e-04\n",
      "Epoch 4/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3307WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 179s 310ms/step - loss: 0.3307 - val_loss: 0.3238 - lr: 1.0000e-04\n",
      "Epoch 5/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.3089WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n",
      "578/578 [==============================] - 178s 308ms/step - loss: 0.3089 - val_loss: 0.3836 - lr: 1.0000e-04\n",
      "Epoch 6/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.2641WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
      "578/578 [==============================] - 178s 308ms/step - loss: 0.2641 - val_loss: 0.3270 - lr: 2.5000e-05\n",
      "Epoch 7/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.2470WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 178s 308ms/step - loss: 0.2470 - val_loss: 0.3123 - lr: 1.0000e-05\n",
      "Epoch 8/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.2381WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 176s 305ms/step - loss: 0.2381 - val_loss: 0.3172 - lr: 1.0000e-05\n",
      "Epoch 9/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.2304WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 178s 308ms/step - loss: 0.2304 - val_loss: 0.3214 - lr: 1.0000e-05\n",
      "Epoch 10/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.2238WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 177s 306ms/step - loss: 0.2238 - val_loss: 0.3202 - lr: 1.0000e-05\n",
      "Epoch 11/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.2180WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 176s 304ms/step - loss: 0.2180 - val_loss: 0.3277 - lr: 1.0000e-05\n",
      "Epoch 12/40\n",
      "578/578 [==============================] - ETA: 0s - loss: 0.2113WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "578/578 [==============================] - 176s 305ms/step - loss: 0.2113 - val_loss: 0.3478 - lr: 1.0000e-05\n",
      "[2023_01_03-00:00:24] Training on final epochs of sequence length 1024...\n",
      "[2023_01_03-00:00:24] Training set: Filtered out 0 of 18492 (0.0%) records of lengths exceeding 1022.\n",
      "[2023_01_03-00:00:25] Validation set: Filtered out 0 of 2055 (0.0%) records of lengths exceeding 1022.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 00:00:38.626226: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-01-03 01:27:38.448442: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1156/1156 [==============================] - ETA: 0s - loss: 0.2575WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-03 01:43:45.634018: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1156/1156 [==============================] - 1000s 807ms/step - loss: 0.2575 - val_loss: 0.3206 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained model and fine-tuning it on the loaded dataset\n",
    "\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "\n",
    "# get_model_with_hidden_layers_as_outputs gives the model output access to the hidden layers (on top of the output)\n",
    "model_generator = FinetuningModelGenerator(pretrained_model_generator, OUTPUT_SPEC, pretraining_model_manipulation_function = \\\n",
    "        get_model_with_hidden_layers_as_outputs, dropout_rate = 0.5)\n",
    "\n",
    "training_callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-05, verbose = 1),\n",
    "    keras.callbacks.EarlyStopping(patience = 5, restore_best_weights = True),\n",
    "]\n",
    "\n",
    "finetune(model_generator, input_encoder, OUTPUT_SPEC, X_train, y_train, X_val, y_val, \\\n",
    "        seq_len = 512, batch_size = 32, max_epochs_per_stage = 40, lr = 1e-04, begin_with_frozen_pretrained_layers = True, \\\n",
    "        lr_with_frozen_pretrained_layers = 1e-02, n_final_epochs = 1, final_seq_len = 1024, final_lr = 1e-05, callbacks = training_callbacks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n",
      "2023-01-03 06:29:19.024396: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 25s 119ms/step\n",
      "Test-set performance:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># records</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model seq len</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>512</th>\n",
       "      <td>2283</td>\n",
       "      <td>0.923979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>2283</td>\n",
       "      <td>0.923979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               # records       AUC\n",
       "Model seq len                     \n",
       "512                 2283  0.923979\n",
       "All                 2283  0.923979"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>630</td>\n",
       "      <td>516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0     1\n",
       "0  630   516\n",
       "1    1  1136"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluating the performance on the test-set\n",
    "\n",
    "results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, OUTPUT_SPEC, X_test, y_test, \\\n",
    "        start_seq_len = 512, start_batch_size = 32)\n",
    "\n",
    "print('Test-set performance:')\n",
    "display(results)\n",
    "\n",
    "print('Confusion matrix:')\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022_11_29-12:29:55] ========== signalP_binary ==========\n",
      "[2022_11_29-12:29:55] Output type: global binary\n",
      "[2022_11_29-12:29:56] Validation set /Users/joseph/Desktop/Academics/11th_Grade/STEM/ML/Data/peptide_benchmarks/signalP_binary.valid.csv missing. Splitting training set instead.\n",
      "[2022_11_29-12:29:56] 14945 training set records, 1661 validation set records, 4152 test set records.\n",
      "[2022_11_29-12:29:56] Training set: Filtered out 0 of 14945 (0.0%) records of lengths exceeding 510.\n",
      "[2022_11_29-12:29:56] Validation set: Filtered out 0 of 1661 (0.0%) records of lengths exceeding 510.\n",
      "[2022_11_29-12:29:56] Training with frozen pretrained layers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-29 13:43:20.489162: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468/468 [==============================] - ETA: 0s - loss: 0.1010"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from proteinbert import OutputType, OutputSpec, FinetuningModelGenerator, load_pretrained_model, finetune, evaluate_by_len, log\n",
    "from proteinbert.conv_and_global_attention_model import get_model_with_hidden_layers_as_outputs\n",
    "\n",
    "BENCHMARKS = [\n",
    "    # name, output_type\n",
    "    ('signalP_binary', OutputType(False, 'binary')),\n",
    "    ('fluorescence', OutputType(False, 'numeric')),\n",
    "    ('remote_homology', OutputType(False, 'categorical')),\n",
    "    ('stability', OutputType(False, 'numeric')),\n",
    "    ('scop', OutputType(False, 'categorical')),\n",
    "    ('secondary_structure', OutputType(True, 'categorical')),\n",
    "    ('disorder_secondary_structure', OutputType(True, 'binary')),\n",
    "    ('ProFET_NP_SP_Cleaved', OutputType(False, 'binary')),\n",
    "    ('PhosphositePTM', OutputType(True, 'binary')),\n",
    "]\n",
    "\n",
    "settings = {\n",
    "    'max_dataset_size': None,\n",
    "    'max_epochs_per_stage': 40,\n",
    "    'seq_len': 512,\n",
    "    'batch_size': 32,\n",
    "    'final_epoch_seq_len': 1024,\n",
    "    'initial_lr_with_frozen_pretrained_layers': 1e-02,\n",
    "    'initial_lr_with_all_layers': 1e-04,\n",
    "    'final_epoch_lr': 1e-05,\n",
    "    'dropout_rate': 0.5,\n",
    "    'training_callbacks': [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience = 1, factor = 0.25, min_lr = 1e-05, verbose = 1),\n",
    "        keras.callbacks.EarlyStopping(patience = 2, restore_best_weights = True),\n",
    "    ],\n",
    "}\n",
    "\n",
    "####### Uncomment for debug mode\n",
    "# settings['max_dataset_size'] = 500\n",
    "# settings['max_epochs_per_stage'] = 1\n",
    "\n",
    "def run_benchmark(benchmark_name, pretraining_model_generator, input_encoder, pretraining_model_manipulation_function = None):\n",
    "    \n",
    "    log('========== %s ==========' % benchmark_name)  \n",
    "    \n",
    "    output_type = get_benchmark_output_type(benchmark_name)\n",
    "    log('Output type: %s' % output_type)\n",
    "    \n",
    "    train_set, valid_set, test_set = load_benchmark_dataset(benchmark_name)        \n",
    "    log(f'{len(train_set)} training set records, {len(valid_set)} validation set records, {len(test_set)} test set records.')\n",
    "    \n",
    "    if settings['max_dataset_size'] is not None:\n",
    "        log('Limiting the training, validation and test sets to %d records each.' % settings['max_dataset_size'])\n",
    "        train_set = train_set.sample(min(settings['max_dataset_size'], len(train_set)), random_state = 0)\n",
    "        valid_set = valid_set.sample(min(settings['max_dataset_size'], len(valid_set)), random_state = 0)\n",
    "        test_set = test_set.sample(min(settings['max_dataset_size'], len(test_set)), random_state = 0)\n",
    "    \n",
    "    if output_type.is_seq or output_type.is_categorical:\n",
    "        train_set['label'] = train_set['label'].astype(str)\n",
    "        valid_set['label'] = valid_set['label'].astype(str)\n",
    "        test_set['label'] = test_set['label'].astype(str)\n",
    "    else:\n",
    "        train_set['label'] = train_set['label'].astype(float)\n",
    "        valid_set['label'] = valid_set['label'].astype(float)\n",
    "        test_set['label'] = test_set['label'].astype(float)\n",
    "        \n",
    "    if output_type.is_categorical:\n",
    "        \n",
    "        if output_type.is_seq:\n",
    "            unique_labels = sorted(set.union(*train_set['label'].apply(set)) | set.union(*valid_set['label'].apply(set)) | \\\n",
    "                    set.union(*test_set['label'].apply(set)))\n",
    "        else:\n",
    "            unique_labels = sorted(set(train_set['label'].unique()) | set(valid_set['label'].unique()) | set(test_set['label'].unique()))\n",
    "            \n",
    "        log('%d unique lebels.' % len(unique_labels))\n",
    "    elif output_type.is_binary:\n",
    "        unique_labels = [0, 1]\n",
    "    else:\n",
    "        unique_labels = None\n",
    "        \n",
    "    output_spec = OutputSpec(output_type, unique_labels)\n",
    "    model_generator = FinetuningModelGenerator(pretraining_model_generator, output_spec, pretraining_model_manipulation_function = \\\n",
    "            pretraining_model_manipulation_function, dropout_rate = settings['dropout_rate'])\n",
    "    finetune(model_generator, input_encoder, output_spec, train_set['seq'], train_set['label'], valid_set['seq'], valid_set['label'], \\\n",
    "            seq_len = settings['seq_len'], batch_size = settings['batch_size'], max_epochs_per_stage = settings['max_epochs_per_stage'], \\\n",
    "            lr = settings['initial_lr_with_all_layers'], begin_with_frozen_pretrained_layers = True, lr_with_frozen_pretrained_layers = \\\n",
    "            settings['initial_lr_with_frozen_pretrained_layers'], n_final_epochs = 1, final_seq_len = settings['final_epoch_seq_len'], \\\n",
    "            final_lr = settings['final_epoch_lr'], callbacks = settings['training_callbacks'])\n",
    "    \n",
    "    for dataset_name, dataset in [('Training-set', train_set), ('Validation-set', valid_set), ('Test-set', test_set)]:\n",
    "        \n",
    "        log('*** %s performance: ***' % dataset_name)\n",
    "        results, confusion_matrix = evaluate_by_len(model_generator, input_encoder, output_spec, dataset['seq'], dataset['label'], \\\n",
    "                start_seq_len = settings['seq_len'], start_batch_size = settings['batch_size'])\n",
    "    \n",
    "        with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "            display(results)\n",
    "        \n",
    "        if confusion_matrix is not None:\n",
    "            with pd.option_context('display.max_rows', 16, 'display.max_columns', 10):\n",
    "                log('Confusion matrix:')\n",
    "                display(confusion_matrix)\n",
    "                \n",
    "    return model_generator\n",
    "\n",
    "def load_benchmark_dataset(benchmark_name):\n",
    "    \n",
    "    train_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.train.csv' % benchmark_name)\n",
    "    valid_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.valid.csv' % benchmark_name)\n",
    "    test_set_file_path = os.path.join(BENCHMARKS_DIR, '%s.test.csv' % benchmark_name)\n",
    "    \n",
    "    train_set = pd.read_csv(train_set_file_path).dropna().drop_duplicates()\n",
    "    test_set = pd.read_csv(test_set_file_path).dropna().drop_duplicates()\n",
    "          \n",
    "    if os.path.exists(valid_set_file_path):\n",
    "        valid_set = pd.read_csv(valid_set_file_path).dropna().drop_duplicates()\n",
    "    else:\n",
    "        log(f'Validation set {valid_set_file_path} missing. Splitting training set instead.')\n",
    "        train_set, valid_set = train_test_split(train_set, stratify = train_set['label'], test_size = 0.1, random_state = 0)\n",
    "    \n",
    "    return train_set, valid_set, test_set\n",
    "\n",
    "def get_benchmark_output_type(benchmark_name):\n",
    "    for name, output_type in BENCHMARKS:\n",
    "        if name == benchmark_name:\n",
    "            return output_type\n",
    "        \n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "\n",
    "for benchmark_name, _ in BENCHMARKS:\n",
    "    run_benchmark(benchmark_name, pretrained_model_generator, input_encoder, pretraining_model_manipulation_function = \\\n",
    "            get_model_with_hidden_layers_as_outputs)\n",
    "        \n",
    "log('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the attention layers\n",
    "\n",
    "You can run this only after you have fine-tuned the model on a benchmark (e.g. signal peptide) and obtained *model_generator*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'seq'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'seq'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 56\u001b[0m\n\u001b[1;32m     52\u001b[0m                 ax\u001b[39m.\u001b[39mtext(i \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m, j \u001b[39m+\u001b[39m \u001b[39m0.5\u001b[39m, plus_sign \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (\u001b[39m100\u001b[39m \u001b[39m*\u001b[39m value), color \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mwhite\u001b[39m\u001b[39m'\u001b[39m, ha \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m'\u001b[39m, va \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcenter\u001b[39m\u001b[39m'\u001b[39m, \\\n\u001b[1;32m     53\u001b[0m                         fontsize \u001b[39m=\u001b[39m \u001b[39m9\u001b[39m, fontweight \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbold\u001b[39m\u001b[39m'\u001b[39m, fontstretch \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcondensed\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     55\u001b[0m test_set \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(TEST_SET_FILE_PATH)\n\u001b[0;32m---> 56\u001b[0m chosen_index \u001b[39m=\u001b[39m ((test_set[\u001b[39m'\u001b[39;49m\u001b[39mseq\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mlen() \u001b[39m-\u001b[39m IDEAL_LEN)\u001b[39m.\u001b[39mabs())\u001b[39m.\u001b[39msort_values()\u001b[39m.\u001b[39mindex[\u001b[39m0\u001b[39m]\n\u001b[1;32m     57\u001b[0m seq \u001b[39m=\u001b[39m test_set\u001b[39m.\u001b[39mloc[chosen_index, \u001b[39m'\u001b[39m\u001b[39mseq\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     58\u001b[0m label \u001b[39m=\u001b[39m test_set\u001b[39m.\u001b[39mloc[chosen_index, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/pandas/core/frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniforge3/envs/test_env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'seq'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BENCHMARK_DISPLAY_NAME = 'Signal peptide'\n",
    "\n",
    "TEST_SET_FILE_PATH = '../Data/ProtTrans_Finetune/healthy_vs_covid_test.csv'\n",
    "IDEAL_LEN = 100\n",
    "\n",
    "def calculate_attentions(model, input_encoder, seq, seq_len = None):\n",
    "    \n",
    "    from tensorflow.keras import backend as K\n",
    "    from proteinbert.tokenization import index_to_token\n",
    "    \n",
    "    if seq_len is None:\n",
    "        seq_len = len(seq) + 2\n",
    "    \n",
    "    X = input_encoder.encode_X([seq], seq_len)\n",
    "    (X_seq,), _ = X\n",
    "    seq_tokens = list(map(index_to_token.get, X_seq))\n",
    "\n",
    "    model_inputs = [layer.input for layer in model.layers if 'InputLayer' in str(type(layer))][::-1]\n",
    "    model_attentions = [layer.calculate_attention(layer.input) for layer in model.layers if 'GlobalAttention' in str(type(layer))]\n",
    "    invoke_model_attentions = K.function(model_inputs, model_attentions)\n",
    "    attention_values = invoke_model_attentions(X)\n",
    "    \n",
    "    attention_labels = []\n",
    "    merged_attention_values = []\n",
    "\n",
    "    for attention_layer_index, attention_layer_values in enumerate(attention_values):\n",
    "        for head_index, head_values in enumerate(attention_layer_values):\n",
    "            attention_labels.append('Attention %d - head %d' % (attention_layer_index + 1, head_index + 1))\n",
    "            merged_attention_values.append(head_values)\n",
    "\n",
    "    attention_values = np.array(merged_attention_values)\n",
    "    \n",
    "    return attention_values, seq_tokens, attention_labels\n",
    "\n",
    "def plot_attention(attention_values, seq_tokens, attention_labels, ax, cmap = 'Reds', vmin = 0, vmax = None, text_value_threshold = 0.1):\n",
    "\n",
    "    heatmap = ax.pcolor(attention_values.transpose(), cmap = cmap, vmin = vmin, vmax = vmax)\n",
    "\n",
    "    ax.set_xticks(np.arange(len(attention_labels)) + 0.5)\n",
    "    ax.set_xticklabels(attention_labels, rotation = 45, ha = 'right', fontsize = 12)\n",
    "    ax.set_yticks(np.arange(len(seq_tokens)) + 0.5)\n",
    "    ax.set_yticklabels(seq_tokens, fontsize = 12)\n",
    "\n",
    "    for i, row in enumerate(attention_values):\n",
    "        for j, value in enumerate(row):\n",
    "            if abs(value) >= text_value_threshold:\n",
    "                add_plus_sign = attention_values.min() < 0 and value > 0\n",
    "                plus_sign = '+' if add_plus_sign else ''\n",
    "                ax.text(i + 0.5, j + 0.5, plus_sign + '%d%%' % (100 * value), color = 'white', ha = 'center', va = 'center', \\\n",
    "                        fontsize = 9, fontweight = 'bold', fontstretch = 'condensed')\n",
    "                \n",
    "test_set = pd.read_csv(TEST_SET_FILE_PATH)\n",
    "chosen_index = ((test_set['seq'].str.len() - IDEAL_LEN).abs()).sort_values().index[0]\n",
    "seq = test_set.loc[chosen_index, 'seq']\n",
    "label = test_set.loc[chosen_index, 'label']\n",
    "                \n",
    "seq_len = len(seq) + 2\n",
    "\n",
    "pretrained_model_generator, input_encoder = load_pretrained_model()\n",
    "model = pretrained_model_generator.create_model(seq_len)\n",
    "pretrained_attention_values, pretrained_seq_tokens, pretrained_attention_labels = calculate_attentions(model, input_encoder, seq, \\\n",
    "        seq_len = seq_len)\n",
    "\n",
    "model = model_generator.create_model(seq_len)\n",
    "finetuned_attention_values, finetuned_seq_tokens, finetuned_attention_labels = calculate_attentions(model, input_encoder, seq, \\\n",
    "        seq_len = seq_len)\n",
    "assert finetuned_seq_tokens == pretrained_seq_tokens\n",
    "assert finetuned_attention_labels == pretrained_attention_labels[:len(finetuned_attention_labels)]\n",
    "\n",
    "fig, axes = plt.subplots(ncols = 4, figsize = (20, 0.2 * seq_len), gridspec_kw = dict(width_ratios = [1, 5, 1, 5]))\n",
    "fig.subplots_adjust(wspace = 0.3)\n",
    "\n",
    "axes[0].barh(np.arange(seq_len), 100 * pretrained_attention_values.sum(axis = 0), color = '#EC7063')\n",
    "axes[0].set_ylim((-0.5, seq_len - 0.5))\n",
    "axes[0].set_yticks([])\n",
    "axes[0].invert_xaxis()\n",
    "axes[0].set_xlabel('Total atten. %', fontsize = 14)\n",
    "\n",
    "vmax = pretrained_attention_values.max()\n",
    "plot_attention(pretrained_attention_values, pretrained_seq_tokens, pretrained_attention_labels, axes[1], cmap = 'Reds', vmax = vmax, \\\n",
    "        text_value_threshold = 0.05)\n",
    "axes[1].set_title('Only pre-training', fontsize = 16)\n",
    "\n",
    "axes[2].barh(np.arange(seq_len), 100 * (finetuned_attention_values - pretrained_attention_values).sum(axis = 0), color = '#28B463')\n",
    "axes[2].set_ylim((-0.5, seq_len - 0.5))\n",
    "axes[2].set_yticks([])\n",
    "axes[2].invert_xaxis()\n",
    "axes[2].set_xlabel('Total atten. % diff', fontsize = 14)\n",
    "\n",
    "attention_diff = finetuned_attention_values - pretrained_attention_values[:len(finetuned_attention_labels), :]\n",
    "vmax = np.abs(attention_diff).max()\n",
    "plot_attention(attention_diff, finetuned_seq_tokens, finetuned_attention_labels, axes[3], cmap = 'PiYG', vmin = -vmax, vmax = vmax, \\\n",
    "        text_value_threshold = 0.03)\n",
    "axes[3].set_title('%s fine-tuning' % BENCHMARK_DISPLAY_NAME, fontsize = 16)\n",
    "\n",
    "print(seq, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('test_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:25:13) [Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c38c8779811d1cfaf4b5a784c97578f212c26cffc36ab1ef679f872ba1fdba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
