{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
    "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from Bio.Seq import Seq\n",
    "from transformers import TFBertModel, BertTokenizer,BertConfig\n",
    "import re\n",
    "import pickle\n",
    "import sys\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# np.random.seed(42)\n",
    "# tf.random.set_seed(42)\n",
    "# os.environ['PYTHONHASHSEED']=str(42)\n",
    "tf.keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd\", do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "550d85f1-6320-432d-8b20-b6531a0fc845",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VHorVHH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EVQLVESGGGLIQPGGSLRLSCAASGLTVSSNYMNWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EVQLVQSGGGLVQPGGSLRLSCLASGLTFSSYEFNWIRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QVQLVQSGAEVKRPGASVKVLCMASGYSFTNYGINWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EVQLVQSGAEVKKPRESLKISCKGSGYNFTSYWIGWVRQMPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASRFTFANYWMSWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11999</th>\n",
       "      <td>EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12000</th>\n",
       "      <td>QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12001</th>\n",
       "      <td>QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12002</th>\n",
       "      <td>EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12003</th>\n",
       "      <td>QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11538 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 VHorVHH\n",
       "0      EVQLVESGGGLIQPGGSLRLSCAASGLTVSSNYMNWVRQAPGKGLE...\n",
       "1      EVQLVQSGGGLVQPGGSLRLSCLASGLTFSSYEFNWIRQAPGKGLE...\n",
       "2      QVQLVQSGAEVKRPGASVKVLCMASGYSFTNYGINWVRQAPGQGLE...\n",
       "3      EVQLVQSGAEVKKPRESLKISCKGSGYNFTSYWIGWVRQMPGKGLE...\n",
       "4      EVQLVESGGGLVQPGGSLRLSCAASRFTFANYWMSWVRQAPGKGLE...\n",
       "...                                                  ...\n",
       "11999  EVQVVESGGGLVKPGGSLRLSCAASGFTFSSYTMNWVRQAPGKGLE...\n",
       "12000  QMQLVQSGPEVKRPGTSVKVSCEASGFTFSSSAILWVRQPRGQRLE...\n",
       "12001  QVQLVESGGGLVKPGGSLRLSCAASGFTFSDYYMNWIRQAPGKGLE...\n",
       "12002  EVQLVESGGGLVQPGGSLRLSCAASGFTFSRFAMHWVRQAPGKGLE...\n",
       "12003  QVQLVQSGAEVKKPGTSMRVSCKASGYTFSTYGIIWVRQAPGQGLE...\n",
       "\n",
       "[11538 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Data/CoV-AbDab_201222.csv\")\n",
    "df = df[[\"VHorVHH\"]]\n",
    "df = df[df[\"VHorVHH\"].apply(lambda x: len(x) <= 138)]\n",
    "df = df[(df.VHorVHH != 'ND')]\n",
    "df\n",
    "# df = df[[\"CDRH3\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca827dc2-3d2e-4bbd-80ae-08b1ea26e6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/joseph/miniforge3/envs/test_env/lib/python3.10/site-packages/Bio/Seq.py:3482: BiopythonWarning: Partial codon, len(sequence) not a multiple of three. Explicitly trim the sequence or add trailing N before translation. This may become an error in future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EVQLVQSGPEVKKPGSSVKVSCKASGGTFSNFAFSWVRQAPGQGLEWMGSVILHLGTSTYAQKFQGRVTITADESTSAAFMDLNALTSDDTAVYYCARVVAVPGRVPYWFDPWGQGTLVTVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEINHSGSTNYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARVPPTSTVTTLGDDYWGQGTLVTVSS', 'QVQLVQSGPEVKKPGASVRVSCKPSGYPFSNYGISWMRQAPGQGLEWMGWVNIDKGNTKYAQKFQDRVTMTTDTSSSTVYLELRSLRSDDTALYYCARERGGYRYGDYWGQGTLVIVSS', 'TLSLTCAVYGGSFSGYYWSWIRQPPGKGLEWIGEIKHSGSTNYIPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCASRAGAAAASWGQGTLVTVSS', 'SETLSLTCAVHGGSFSDYYWTWIRQPPGKGLEWIGEINHRGGTNYNPSLKSRLNILVDTSKSQFSLKLSSVTAADTAVYFCARERFILIRGLTKYYYYMDVWGKGTTVTVS'] 11538\n"
     ]
    }
   ],
   "source": [
    "dummy = []\n",
    "head = []\n",
    "with open(\"../Data/cAb-rep/cAb-Rep_heavy.nt.txt\") as myfile:\n",
    "    # count = 0\n",
    "    for i in myfile:\n",
    "        # if count <= 1:\n",
    "        #     print(i)\n",
    "        #     if i.find(\">\") == -1 & i.find(\"-\") == -1:\n",
    "        #         print(Seq.translate(i.strip()))\n",
    "        #     count+=1\n",
    "        dummy.append(i)\n",
    "    np.random.shuffle(dummy)\n",
    "    \n",
    "    for i in dummy:\n",
    "        if i.find(\">\") == -1 & i.find(\"-\") == -1 & i.find(\"N\") == -1: # These conditions must be met for a valid sequence, the longest was 141. However, there is no 141 sequence for COVID, the greatest is 138, so we go with that\n",
    "            aa_sequence = Seq.translate(i.strip())\n",
    "            if (len(aa_sequence) <= 138) & (len(aa_sequence) >= 100):\n",
    "                head.append(aa_sequence)\n",
    "                if len(head) >= 11538:\n",
    "                    break\n",
    "print(head[:5], len(head))\n",
    "healthy_sequences = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3c0779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del head\n",
    "del myfile\n",
    "del dummy\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67483afe-f027-4dd9-9d03-e62e4145b446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275\n",
      "275\n"
     ]
    }
   ],
   "source": [
    "covid_sequences = df.to_numpy()\n",
    "covid_sequences = np.squeeze(covid_sequences)\n",
    "np.random.shuffle(covid_sequences)\n",
    "healthy_sequences = np.array([re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in healthy_sequences])\n",
    "healthy_sequences = np.array([(\" \".join(s)) for s in healthy_sequences])\n",
    "covid_sequences = np.array([re.sub(r\"[UZOB]\", \"X\", sequence) for sequence in covid_sequences])\n",
    "covid_sequences = np.array([(\" \".join(s)) for s in covid_sequences])\n",
    "\n",
    "print(len(max(healthy_sequences, key=len)))\n",
    "print(len(max(covid_sequences, key=len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['E V Q L V Q S G P E V K K P G S S V K V S C K A S G G T F S N F A F S W V R Q A P G Q G L E W M G S V I L H L G T S T Y A Q K F Q G R V T I T A D E S T S A A F M D L N A L T S D D T A V Y Y C A R V V A V P G R V P Y W F D P W G Q G T L V T V S S'\n",
      " 'T L S L T C A V Y G G S F S G Y Y W S W I R Q P P G K G L E W I G E I N H S G S T N Y N P S L K S R V T I S V D T S K N Q F S L K L S S V T A A D T A V Y Y C A R V P P T S T V T T L G D D Y W G Q G T L V T V S S'\n",
      " 'Q V Q L V Q S G P E V K K P G A S V R V S C K P S G Y P F S N Y G I S W M R Q A P G Q G L E W M G W V N I D K G N T K Y A Q K F Q D R V T M T T D T S S S T V Y L E L R S L R S D D T A L Y Y C A R E R G G Y R Y G D Y W G Q G T L V I V S S'\n",
      " ...\n",
      " 'Q S G A E V K K A G E S L R I S C K A S G Y S F A S Y W I G W V R Q M P G K G L E C M G I I N P G D S D T R Y S P S F Q G H V T I S V D K S V N T A Y L Q W S S L K A S D T A I Y N C S K Q I I T Y S S G W Y G F D Y W G Q G T L V T V S'\n",
      " 'D V Q L L E S G G G L A Q P G G S L R L S C A A S G F N F N N Y A M S W V R Q A P G K G L E W V S A I S G G A D D T Y Y A E S V K G R F I I S R D N S K R K V F L Q L T S L A A E D T A L Y F C A R R G A T V T E G H G G F F D H W G Q G T L V N V S S'\n",
      " 'Q V Q L V Q S G A E V K K P G S S V K V S C K A S G G T F S S Y A I S W V R Q A P G Q G L E W M G G I I P I F G T A N Y A Q K F Q G R V T I T A D E S T S T A Y M E L S S L R S E D T A V Y Y C A R S V D T A M V P V Y Y Y Y Y Y M D V W G K G T T V T V S']\n"
     ]
    }
   ],
   "source": [
    "print(healthy_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Max\n",
      "\n",
      "systemMemory: 64.00 GB\n",
      "maxCacheSize: 24.00 GB\n",
      "\n",
      "tf.Tensor(\n",
      "[[ 2  9  8 ...  0  0  0]\n",
      " [ 2 15  5 ...  0  0  0]\n",
      " [ 2 18  8 ...  0  0  0]\n",
      " ...\n",
      " [ 2 18 10 ...  0  0  0]\n",
      " [ 2 14  8 ...  0  0  0]\n",
      " [ 2 18  8 ...  0  0  0]], shape=(11538, 140), dtype=int32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 22:31:18.581630: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-01-17 22:31:18.581753: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.batch_encode_plus(healthy_sequences, add_special_tokens=True, padding=True, return_tensors=\"tf\")\n",
    "healthy_tokens = ids['input_ids']\n",
    "print(healthy_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2 18  8 ...  0  0  0]\n",
      " [ 2 18  8 ...  0  0  0]\n",
      " [ 2 18  8 ...  0  0  0]\n",
      " ...\n",
      " [ 2 18  8 ...  0  0  0]\n",
      " [ 2 18  8 ...  0  0  0]\n",
      " [ 2 18  8 ...  0  0  0]], shape=(11538, 140), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer.batch_encode_plus(covid_sequences, add_special_tokens=True, padding=True, return_tensors=\"tf\")\n",
    "covid_tokens = ids['input_ids']\n",
    "print(covid_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_lables = [0] * 11538\n",
    "covid_lables = [1] * 11538"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate((healthy_tokens, covid_tokens))\n",
    "y = np.concatenate((healthy_lables, covid_lables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7452eec8-9c73-452a-b112-92a653d1fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22372ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X\n",
    "del y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab_size = 20000  # Only consider the top 20k words\n",
    "# maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "# (x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
    "# print(len(x_train), \"Training sequences\")\n",
    "# print(len(x_val), \"Validation sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = tf.keras.utils.pad_sequences(x_train, maxlen=maxlen)\n",
    "# x_val = tf.keras.utils.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x_train.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Build and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure borrowed from https://keras.io/examples/nlp/text_classification_with_transformer/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [Dense(ff_dim, activation=\"relu\"), \n",
    "             Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 1024  # Embedding size for each token\n",
    "num_heads = 16  # Number of attention heads\n",
    "ff_dim = 512  # Hidden layer size in feed forward network inside transformer\n",
    "maxlen = 140\n",
    "vocab_size = 30\n",
    "\n",
    "# x6 greater\n",
    "\n",
    "inputs = Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = GlobalAveragePooling1D()(x)\n",
    "x = Dropout(0.2, seed=42)(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dropout(0.3, seed=42)(x)\n",
    "outputs = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 140)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 140, 1024)        174080    \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 140, 1024)        68213248  \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 1024)             0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 64)                16448     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 68,666,241\n",
      "Trainable params: 68,666,241\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 22:31:25.785465: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-01-17 22:31:26.470490: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585/585 [==============================] - ETA: 0s - loss: 0.5519 - accuracy: 0.7042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 23:19:41.157358: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585/585 [==============================] - 3015s 5s/step - loss: 0.5519 - accuracy: 0.7042 - val_loss: 0.5660 - val_accuracy: 0.7106\n",
      "Epoch 2/10\n",
      "585/585 [==============================] - 3020s 5s/step - loss: 0.4972 - accuracy: 0.7370 - val_loss: 0.4909 - val_accuracy: 0.7366\n",
      "Epoch 3/10\n",
      "585/585 [==============================] - 2936s 5s/step - loss: 0.4911 - accuracy: 0.7449 - val_loss: 0.4896 - val_accuracy: 0.7386\n",
      "Epoch 4/10\n",
      "585/585 [==============================] - 2938s 5s/step - loss: 0.4725 - accuracy: 0.7493 - val_loss: 0.4589 - val_accuracy: 0.7482\n",
      "Epoch 5/10\n",
      "585/585 [==============================] - 2934s 5s/step - loss: 0.4569 - accuracy: 0.7534 - val_loss: 0.4568 - val_accuracy: 0.7540\n",
      "Epoch 6/10\n",
      "585/585 [==============================] - 2932s 5s/step - loss: 0.4489 - accuracy: 0.7599 - val_loss: 0.4324 - val_accuracy: 0.7622\n",
      "Epoch 7/10\n",
      "585/585 [==============================] - 2925s 5s/step - loss: 0.4434 - accuracy: 0.7628 - val_loss: 0.4374 - val_accuracy: 0.7622\n",
      "Epoch 8/10\n",
      "585/585 [==============================] - 2928s 5s/step - loss: 0.4430 - accuracy: 0.7677 - val_loss: 0.4249 - val_accuracy: 0.7718\n",
      "Epoch 9/10\n",
      "585/585 [==============================] - 2932s 5s/step - loss: 0.4398 - accuracy: 0.7669 - val_loss: 0.4358 - val_accuracy: 0.7766\n",
      "Epoch 10/10\n",
      "585/585 [==============================] - 2930s 5s/step - loss: 0.4401 - accuracy: 0.7686 - val_loss: 0.4267 - val_accuracy: 0.7752\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=32, epochs=10, \n",
    "                    validation_data=(X_val, y_val)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 - 129s - loss: 0.4441 - accuracy: 0.7660 - 129s/epoch - 2s/step\n",
      "loss: 0.4441\n",
      "accuracy: 0.7660\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "for name, value in zip(model.metrics_names, results):\n",
    "    print(\"%s: %.4f\" % (name, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c38c8779811d1cfaf4b5a784c97578f212c26cffc36ab1ef679f872ba1fdba43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
